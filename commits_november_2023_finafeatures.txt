diff --git a/R/strategy_momentum_ml_v3.R b/R/strategy_momentum_ml_v3.R
new file mode 100644
index 0000000..7712cb0
--- /dev/null
+++ b/R/strategy_momentum_ml_v3.R
@@ -0,0 +1,921 @@
+library(data.table)
+library(tiledb)
+library(rvest)
+library(lubridate)
+library(finfeatures)
+library(mlr3)
+library(mlr3pipelines)
+library(mlr3viz)
+library(mlr3tuning)
+library(mlr3mbo)
+library(mlr3misc)
+library(mlr3hyperband)
+library(mlr3extralearners)
+library(bbotk)
+library(future.apply)
+library(QuantTools)
+library(PerformanceAnalytics)
+
+
+
+# SET UP ------------------------------------------------------------------
+# globals
+DATAPATH     = "F:/lean_root/data/all_stocks_daily.csv"
+DATAPATHHOUR = "F:/lean_root/data/all_stocks_hour.csv"
+# URIEXUBER    = "F:/equity-usa-hour-exuber"
+NASPATH      = "C:/Users/Mislav/SynologyDrive/trading_data"
+
+
+# UTILS -------------------------------------------------------------------
+# utils https://stackoverflow.com/questions/1995933/number-of-months-between-two-dates
+monnb <- function(d) {
+  lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
+  lt$year*12 + lt$mon }
+mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
+
+
+# UNIVERSE ----------------------------------------------------------------
+# SPY constitues
+spy_const = fread(file.path(NASPATH, "spy.csv"))
+symbols_spy = unique(spy_const)
+
+# SP 500
+sp500_changes = read_html("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies") %>%
+  html_elements("table") %>%
+  .[[2]] %>%
+  html_table()
+sp500_changes = sp500_changes[-1, c(1, 2, 4)]
+sp500_changes = as.data.table(sp500_changes)
+sp500_changes[, Date := as.Date(Date, format = "%b %d, %Y")]
+sp500_changes = sp500_changes[Date < as.Date("2009-06-28")]
+sp500_changes_symbols = unlist(sp500_changes[, 2:3], use.names = FALSE)
+sp500_changes_symbols = unique(sp500_changes_symbols)
+sp500_changes_symbols = sp500_changes_symbols[sp500_changes_symbols != ""]
+
+# SPY constitues + SP 500
+symbols_sp500 = unique(c(symbols_spy, sp500_changes_symbols))
+symbols_sp500 = tolower(symbols_sp500)
+symbols_sp500 = c("spy", symbols_sp500)
+
+
+
+# IMPORT DATA -------------------------------------------------------------
+# import QC daily data
+col = c("date", "open", "high", "low", "close", "volume", "close_adj", "symbol")
+dt = fread(DATAPATH, col.names = col)
+dt <- unique(dt, by = c("symbol", "date"))
+unadjustd_cols = c("open", "high", "low")
+adjusted_cols = paste0(unadjustd_cols, "_adj")
+dt[, (adjusted_cols) := lapply(.SD, function(x) (close_adj / close) * x), .SDcols = unadjustd_cols]
+dt = na.omit(dt)
+setorder(dt, symbol, date)
+
+# import QC hourly data
+# col = c("date", "open", "high", "low", "close", "volume", "close_adj", "symbol")
+# dth = fread(DATAPATHHOUR, col.names = col)
+# dth <- unique(dth, by = c("symbol", "date"))
+# unadjustd_cols = c("open", "high", "low")
+# adjusted_cols = paste0(unadjustd_cols, "_adj")
+# dth[, (adjusted_cols) := lapply(.SD, function(x) (close_adj / close) * x), .SDcols = unadjustd_cols]
+# dth = na.omit(dth)
+# setorder(dth, symbol, date)
+
+# free resources
+gc()
+
+
+# PREDICTORS --------------------------------------------------------------
+# create help colummns
+dt[, year_month_id := ceiling_date(date, unit = "month") - days(1)]
+
+# remove negative prices
+dt = dt[close > 0 & close_adj > 0]
+
+# add variables
+dt[, dollar_volume := volume * close]
+
+# PRA window sizes
+windows_ = c(5, 22, 22 * 3, 22 * 6, 252, 252 * 2, 252 * 4)
+
+# calculate PRA predictors
+pra_predictors = paste0("pra_", windows_)
+dt[, (pra_predictors) := lapply(windows_,
+                                function(w) roll_percent_rank(close_adj, w)),
+   by = symbol]
+
+# calculate momentum indicators
+setorder(dt, symbol, date)
+moms = seq(21, 21 * 12, 21)
+mom_cols = paste0("mom", 1:12)
+dt[, (mom_cols) := lapply(moms, function(x) close_adj  / shift(close_adj , x) - 1), by = symbol]
+moms = seq(21 * 2, 21 * 12, 21)
+mom_cols_lag = paste0("mom_lag", 2:12)
+dt[, (mom_cols_lag) := lapply(moms, function(x) shift(close_adj, 21) / shift(close_adj , x) - 1), by = symbol]
+
+# create target var
+moms_target = c(5, 10, 21, 42, 63)
+moms_target_cols = paste0("mom_target_", c("w", "2w", "m", "2m", "3m"))
+dt[, (moms_target_cols) := lapply(moms_target, function(x) (shift(close_adj , -x, "shift") / close_adj ) - 1),
+   by = symbol]
+
+# order data
+setorder(dt, symbol, date)
+
+# downsample
+dtm = dt[, .SD[.N], by = c("symbol", "year_month_id")]
+dtm_2 = dt[, .(
+  dollar_volume_sum = sum(dollar_volume, na.rm = TRUE),
+  dollar_volume_mean = mean(dollar_volume, na.rm = TRUE),
+  volume_sum = sum(volume, na.rm = TRUE),
+  volume_mean = mean(volume, na.rm = TRUE)
+), by = c("symbol", "year_month_id")]
+dtm = dtm_2[dtm, on = c("symbol", "year_month_id")]
+dtm[symbol == "aapl"]
+
+# plots
+plot(dtm[symbol == "aapl", .(date, dollar_volume)],
+     type = "l", main = "Volume and dolalr volme")
+plot(dtm[symbol == "aapl", .(date, close_adj)],
+     type = "l", main = "Volume and dolalr volme")
+
+
+
+# PRA INDICATORS ----------------------------------------------------------
+# pra indicators
+cols = c("symbol", "date", pra_predictors)
+pra = dt[, ..cols]
+
+# crate dummy variables
+cols <- paste0("pra_", windows_)
+cols_above_999 <- paste0("pr_above_dummy_", windows_)
+pra[, (cols_above_999) := lapply(.SD, function(x) ifelse(x > 0.999, 1, 0)), .SDcols = cols]
+cols_below_001 <- paste0("pr_below_dummy_", windows_)
+pra[, (cols_below_001) := lapply(.SD, function(x) ifelse(x < 0.001, 1, 0)), .SDcols = cols]
+cols_net_1 <- paste0("pr_below_dummy_net_", windows_)
+pra[, (cols_net_1) := pra[, ..cols_above_999] - pra[, ..cols_below_001]]
+
+cols_above_99 <- paste0("pr_above_dummy_99_", windows_)
+pra[, (cols_above_99) := lapply(.SD, function(x) ifelse(x > 0.99, 1, 0)), .SDcols = cols]
+cols_below_01 <- paste0("pr_below_dummy_01_", windows_)
+pra[, (cols_below_01) := lapply(.SD, function(x) ifelse(x < 0.01, 1, 0)), .SDcols = cols]
+cols_net_2 <- paste0("pr_below_dummy_net_0199", windows_)
+pra[, (cols_net_2) := pra[, ..cols_above_99] - pra[, ..cols_below_01]]
+
+cols_above_97 <- paste0("pr_above_dummy_97_", windows_)
+pra[, (cols_above_97) := lapply(.SD, function(x) ifelse(x > 0.97, 1, 0)), .SDcols = cols]
+cols_below_03 <- paste0("pr_below_dummy_03_", windows_)
+pra[, (cols_below_03) := lapply(.SD, function(x) ifelse(x < 0.03, 1, 0)), .SDcols = cols]
+cols_net_3 <- paste0("pr_below_dummy_net_0397", windows_)
+pra[, (cols_net_3) := pra[, ..cols_above_97] - pra[, ..cols_below_03]]
+
+cols_above_95 <- paste0("pr_above_dummy_95_", windows_)
+pra[, (cols_above_95) := lapply(.SD, function(x) ifelse(x > 0.95, 1, 0)), .SDcols = cols]
+cols_below_05 <- paste0("pr_below_dummy_05_", windows_)
+pra[, (cols_below_05) := lapply(.SD, function(x) ifelse(x < 0.05, 1, 0)), .SDcols = cols]
+cols_net_4 <- paste0("pr_below_dummy_net_0595", windows_)
+pra[, (cols_net_4) := pra[, ..cols_above_95] - pra[, ..cols_below_05]]
+
+# PRA indicators
+indicators <- pra[symbol != "SPY", lapply(.SD, sum, na.rm = TRUE),
+                  .SDcols = c(colnames(pra)[grep("pr_\\d+", colnames(pra))],
+                              cols_above_999, cols_above_99, cols_below_001, cols_below_01,
+                              cols_above_97, cols_below_03, cols_above_95, cols_below_05,
+                              cols_net_1, cols_net_2, cols_net_3, cols_net_4),
+                  by = .(date)]
+indicators <- unique(indicators, by = c("date"))
+setorder(indicators, "date")
+
+# free memory
+rm(pra)
+gc()
+
+# create indicator signals
+cols_ = colnames(indicators)[grep("below_dummy_\\d+", colnames(indicators))]
+cols_ = c("date", cols_)
+indicators_below = indicators[, ..cols_]
+q = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)
+indicators_signals = indicators_below[, lapply(.SD, function(x) sapply(quantile(x, probs = q), function(y) x < y)),
+                                      .SDcols = setdiff(cols_, "date")]
+indicators_signals = cbind(date = indicators[, date], indicators_signals)
+setnames(indicators_signals, gsub("\\.", "_", colnames(indicators_signals)))
+
+
+# BACKTEST OPTIMIZATION ---------------------------------------------------
+# parameters
+num_coarse = c(50, 100, 200, 300, 500, 1000, 2000) # number of stocks we leave after coarse universe
+filter_var = "dollar_volume_mean"                  # variable we use for filtering in coarse universe
+min_price = c(1, 2, 5, 10)                         # minimal unadjusted price of the stock at month
+num_long = c(10, 30, 50)                         # number of stocks in portfolio
+mom_vars = colnames(dt)[grep("mom\\d+$|lag", colnames(dt))] # momenutm variables we apply ranking on
+target_vars = "mom_target_m" # colnames(dt)[grep("target", colnames(dt))] # target variables
+risk_variables = colnames(indicators_signals)[-1]
+params = expand.grid(num_coarse, filter_var, min_price, num_long, mom_vars,
+                     target_vars, risk_variables,
+                     stringsAsFactors = FALSE)
+colnames(params) = c("num_coarse", "filter_var", "min_price", "num_long",
+                     "mom_vars", "target_variables", "risk_vars")
+
+# momentum backtest function
+strategy_momentum = function(x,
+                             filter_var = "dollar_volume_mean",
+                             num_coarse = 200,
+                             min_price = 1,
+                             num_long = 50,
+                             mom_var = "mom11",
+                             target_variables = "mom_target_m",
+                             risk_var = "pr_below_dummy_66",
+                             return_cumulative = TRUE) {
+
+  # debug
+  # num_coarse = 50
+  # min_price = 2
+  # num_long = 10
+  # mom_var = "mom10"
+  # target_variables = "mom_target_m"
+  # risk_var = "pr_below_dummy_01_22_5_"
+  # x = copy(dtm)
+
+  # remove missing values
+  cols = c("symbol", "date", "year_month_id", "close", filter_var, mom_var, target_variables)
+  x = na.omit(x[, ..cols])
+
+  # remove assets with price < x$
+  x = x[, .SD[close > min_price], by = year_month_id]
+
+  # filter 500 with highest volume
+  setorderv(x, c("year_month_id", filter_var), order = 1L)
+  x = x[, tail(.SD, num_coarse), by = year_month_id]
+
+  # choose n with highest growth
+  setorderv(x, c("year_month_id", mom_var), order = c(1, -1))
+  y = x[, head(.SD, num_long), by = year_month_id]
+  # y[year_month_id == as.Date("2023-04-30")]
+
+  # get daily data for final universe
+  ret_results = list()
+  yms = y[, unique(year_month_id)]
+  for (i in 1:length(yms)) {
+    ym = yms[i]
+    print(ym)
+    y_sample = y[year_month_id == ym[1]]
+    dt_sample = dt[symbol %chin% y_sample$symbol]
+    dt_sample = dt_sample[year_month_id == ym[1]]
+    cols_ = c("symbol", "date", "year_month_id", "close_adj")
+    dt_sample = dt_sample[, ..cols_]
+    dt_sample[, returns := close_adj / shift(close_adj) - 1]
+    dt_sample = dcast(dt_sample, formula = date ~ symbol, value.var = "returns")
+    dt_sample = indicators_signals[, .(date, risk_var_ = get(risk_var))][dt_sample, on = "date"]
+    dt_sample = dt_sample[shift(risk_var_) == TRUE]
+    if (nrow(dt_sample) == 0) {
+      ret_results[[i]] = NULL
+    } else if (nrow(dt_sample) == 1) {
+      ret_results[[i]] = sum(dt_sample[, 3:ncol(dt_sample)] * 0.1)
+    } else {
+      ret_results[[i]] = Return.portfolio(as.xts.data.table(dt_sample[, .SD, .SDcols = -c("risk_var_")]))
+    }
+  }
+
+
+
+  # # merge y and pra
+  # y_wide = dcast(y, formula = date ~ symbol, value.var = "")
+  # y[indicators, on = "date", roll = -Inf]
+  #
+  # # add daily data to control for risk
+  # y_wide = dcast(y, formula = date ~ symbol, value.var = "")
+  # dt_sample = y_wide[indicators, on = c("date")]
+
+  # get returns by month
+  results = y[, .(returns = sum(get(target_variables) * 1/nrow(.SD))), by = year_month_id]
+
+  # CAR
+  if (return_cumulative) {
+    car = Return.cumulative(as.xts.data.table(results))
+    return(car)
+  } else {
+    return(results)
+  }
+}
+strategy_momentum(dtm, "dollar_volume_mean", 200, 1, 50, "mom11", "mom_target_m") # example
+
+# backtest across all parameters
+results_l = vapply(1:nrow(params), function(i) {
+  strategy_momentum(dtm,
+                    filter_var = params$filter_var[i],
+                    num_coarse = params$num_coarse[i],
+                    min_price = params$min_price[i],
+                    num_long = params$num_long[i],
+                    mom_var = params$mom_vars[i],
+                    target_variables = params$target_variables[i])
+}, FUN.VALUE = numeric(1L))
+results = as.data.table(cbind(params, results_l))
+
+# analyse results
+tail(results[order(results_l), ], 10)
+results[mom_vars == "mom10" & min_price == 2 & num_long == 10 & num_coarse == 50]
+ggplot(results, aes(num_coarse, min_price, fill = results_l)) +
+  geom_tile()
+ggplot(results, aes(num_coarse, num_long, fill = results_l)) +
+  geom_tile()
+ggplot(results, aes(min_price, num_long, fill = results_l)) +
+  geom_tile()
+
+# inspect atom
+strategy_momentum(dtm, "dollar_volume_mean", 50, 2, 10, "mom10", "mom_target_m", TRUE)
+x = strategy_momentum(dtm, "dollar_volume_mean", 50, 2, 10, "mom10", "mom_target_m", FALSE)
+charts.PerformanceSummary(x)
+x[order(returns)]
+
+# compare with QC
+dtm[year_month_id == "2023-05-31"] # same
+symbol_ = "isee"
+dt[symbol == symbol_]
+plot(as.xts.data.table(dt[symbol == symbol_, .(date, close_adj)]), type = "l")
+dtm[symbol == symbol_]
+dtm[symbol == symbol_ & date == "2023-04-28"]
+dtms[symbol == symbol_]
+dtms[symbol == symbol_ & date == "2010-01-31"]
+# Symbol F close_t-n 1.550195415 close_t-1 6.373025595 time 2010-01-01 00:00:00
+# Symbol LVS close_t-n 1.72733057 close_t-1 10.91752951 time 2010-01-29 15:00:00   5.320463
+# Symbol LVS close_t-n 1.6673075 close_t-1 10.31062958 time 2010-02-01  12:00:00   5.184
+6.373025595 / 1.550195415 - 1
+dt[symbol == symbol_ & date %between% c("2023-02-01", "2023-04-02")]
+dt[symbol == symbol_ & date %between% c("2009-01-26", "2009-02-02")]
+dt[symbol == symbol_ & date %between% c(as.Date("2009-12-28") - 270, as.Date("2009-12-28") - 230)]
+dt[symbol == symbol_ & round(close_adj, 3) == 1.727]
+dt[symbol == symbol_ & round(close_adj, 3) == 11.058]
+
+
+# vis results
+charts.PerformanceSummary(portfolio_ret)
+
+# Portfolio results
+SharpeRatio(portfolio_ret)
+SharpeRatio.annualized(portfolio_ret)
+
+
+# inspect
+test = as.data.table(portfolio_ret)
+test[portfolio.returns > 0.5]
+test[portfolio.returns <- -0.5]
+
+
+
+
+
+# UNIVERSE ----------------------------------------------------------------
+# parameters
+num_coarse = 500                  # number of stocks we leave after coarse universe
+filter_var = "dollar_volume_mean" # variable we use for filtering in coarse universe
+min_price = 2                     # minimal unadjusted price of the stock at month
+num_long = 50                     # number of stocks in portfolio
+mom_var = "mom12"                 # momenutm variables we apply ranking on
+target_var = "mom_target_m"       # colnames(dt)[grep("target", colnames(dt))] # target variables
+risk_variable = c("pra", "minmax")
+
+# remove missing values
+cols = c("symbol", "year_month_id", "date", "close", filter_var, mom_var, target_var)
+universe = na.omit(dtm[, ..cols])
+
+# remove assets with price < x$
+universe = universe[, .SD[close > min_price], by = year_month_id]
+
+# filter 500 with highest volume
+setorderv(universe, c("year_month_id", filter_var), order = 1L)
+universe = universe[, tail(.SD, num_coarse), by = year_month_id]
+
+# choose n with highest growth
+setorderv(universe, c("year_month_id", mom_var), order = c(1, -1))
+universe = universe[, head(.SD, num_long), by = year_month_id]
+
+
+
+# PREDICTORS --------------------------------------------------------------
+# create OHLCV object for all symbols in universe
+symbols_in_universe = universe[, unique(symbol)]
+ohlcv = dt[symbol %chin% symbols_in_universe]
+universe[, index := TRUE]
+ohlcv = merge(ohlcv, universe[, .(symbol, date, index)],
+              by = c("symbol", "date"), all.x = TRUE, all.y = FALSE)
+ohlcv[index == TRUE]
+at_ = ohlcv[, which(index == TRUE)]
+lag_ = 1L
+ohlcv = Ohlcv$new(ohlcv[, .(symbol, date, open, high, low, close, volume)],
+                  id_col = "symbol",
+                  date_col = "date",
+                  price = "close",
+                  ohlcv = c("open", "high", "low", "close", "volume"))
+
+# BackCUSUM features
+print("Calculate BackCUSUM features.")
+# at_ = get_at_(RollingBackCusumFeatures)
+RollingBackcusumInit = RollingBackcusum$new(windows = c(22 * 3, 22 * 6),
+                                            workers = 4L,
+                                            at = at_,
+                                            lag = lag_,
+                                            alternative = c("greater", "two.sided"),
+                                            return_power = c(1, 2))
+RollingBackCusumFeatures_new = RollingBackcusumInit$get_rolling_features(ohlcv)
+gc()
+
+# tsfeatures features
+print("Calculate tsfeatures features.")
+# at_ = get_at_(RollingTsfeaturesFeatures)
+RollingTsfeaturesInit = RollingTsfeatures$new(windows = c(22 * 3, 22 * 6),
+                                              workers = 6L,
+                                              at = at_,
+                                              lag = lag_,
+                                              scale = TRUE)
+RollingTsfeaturesFeaturesNew = RollingTsfeaturesInit$get_rolling_features(ohlcv)
+gc()
+
+# theft catch22 features
+print("Calculate Catch22 and feasts features.")
+# at_ = get_at_(RollingTheftCatch22Features)
+RollingTheftInit = RollingTheft$new(windows = c(5, 22, 22 * 3, 22 * 12),
+                                    workers = 4L,
+                                    at = at_,
+                                    lag = lag_,
+                                    features_set = c("catch22", "feasts"))
+RollingTheftCatch22FeaturesNew = RollingTheftInit$get_rolling_features(ohlcv)
+gc()
+
+# ohlcv features
+# Features from OHLLCV
+print("Calculate Ohlcv features.")
+OhlcvFeaturesInit = OhlcvFeatures$new(at = NULL,
+                                      windows = c(5, 10, 22, 22 * 3, 22 * 6, 22 * 12),
+                                      quantile_divergence_window =  c(22, 22*3, 22*6, 22*12, 22*12*2))
+OhlcvFeaturesSet = OhlcvFeaturesInit$get_ohlcv_features(ohlcv)
+OhlcvFeaturesSetSample <- OhlcvFeaturesSet[at_ - lag_]
+nrow(OhlcvFeaturesSetSample) == nrow(universe)
+setorderv(OhlcvFeaturesSetSample, c("symbol", "date"))
+# DEBUG
+head(OhlcvFeaturesSetSample[symbol == "aapl", .(symbol, date)])
+
+# free memory
+rm(OhlcvFeaturesSet)
+gc()
+
+# merge all features test
+rolling_predictors <- Reduce(
+  function(x, y) merge( x, y, by = c("symbol", "date"), all.x = TRUE, all.y = FALSE),
+  list(
+    RollingBackCusumFeatures_new,
+    RollingTheftCatch22FeaturesNew,
+    RollingTsfeaturesFeaturesNew
+  )
+)
+
+# merge ohlcv predictors
+rolling_predictors[, date_rolling := date]
+OhlcvFeaturesSetSample[, date_ohlcv := date]
+predictors <- rolling_predictors[OhlcvFeaturesSetSample, on = c("symbol", "date"), roll = Inf]
+
+# check for duplicates
+predictors[duplicated(predictors[, .(symbol, date)]), .(symbol, date)]
+predictors[duplicated(predictors[, .(symbol, date_ohlcv)]), .(symbol, date_ohlcv)]
+predictors[duplicated(predictors[, .(symbol, date_rolling)]), .(symbol, date_rolling)]
+predictors[duplicated(predictors[, .(symbol, date_rolling)]) | duplicated(predictors[, .(symbol, date_rolling)], fromLast = TRUE),
+         .(symbol, date, date_ohlcv, date_rolling)]
+predictors = predictors[!duplicated(predictors[, .(symbol, date_rolling)])]
+
+# merge features and universe
+any(duplicated(universe[, .(symbol, date)]))
+any(duplicated(predictors[, .(symbol, date_rolling)]))
+predictors[symbol == "aa", .(symbol, date)]
+universe[symbol == "aa", .(symbol, date)]
+predictors = predictors[universe, on = c("symbol", "date"), roll = Inf]
+predictors[, .(symbol, date, date_rolling, date_ohlcv, year_month_id)]
+predictors[duplicated(predictors[, .(symbol, date)]), .(symbol, date)]
+predictors[duplicated(predictors[, .(symbol, date_ohlcv)]), .(symbol, date_ohlcv)]
+predictors[duplicated(predictors[, .(symbol, date_rolling)]), .(symbol, date_rolling)]
+
+#TODO: ADD FUNDAMENTALS
+#TODO ADD MACRODATA
+
+# convert char features to numeric features
+char_cols <- predictors[, colnames(.SD), .SDcols = is.character]
+char_cols <- setdiff(char_cols, c("symbol"))
+predictors[, (char_cols) := lapply(.SD, as.numeric), .SDcols = char_cols]
+
+
+
+# PREDICTOR SPACE ---------------------------------------------------------
+# features space from features raw
+cols_remove <- c("index", "close.y", "dollar_volume_mean")
+cols_non_predictors <- c("symbol", "date", "date_rolling", "date_ohlcv",
+                         "open", "high", "low", "volume", "returns",
+                         "mom_target_m", "year_month_id", "i.close")
+cols_predictors <- setdiff(colnames(predictors), c(cols_remove, cols_non_predictors))
+head(cols_predictors, 10)
+tail(cols_predictors, 500)
+cols <- c(cols_non_predictors, cols_predictors)
+predictors <- predictors[, .SD, .SDcols = cols]
+
+# checks
+predictors[, .(symbol, date, date_rolling, year_month_id)]
+predictors[, .(symbol, date, date_rolling, kurtosis_10)]
+
+
+
+# CLEAN DATA --------------------------------------------------------------
+# convert columns to numeric. This is important only if we import existing features
+dataset <- copy(predictors)
+chr_to_num_cols <- setdiff(colnames(dataset[, .SD, .SDcols = is.character]), c("symbol", "time", "right_time"))
+dataset <- dataset[, (chr_to_num_cols) := lapply(.SD, as.numeric), .SDcols = chr_to_num_cols]
+log_to_num_cols <- colnames(dataset[, .SD, .SDcols = is.logical])
+dataset <- dataset[, (log_to_num_cols) := lapply(.SD, as.numeric), .SDcols = log_to_num_cols]
+
+# remove duplicates
+any(duplicated(dataset[, .(symbol, date)]))
+dataset <- unique(dataset, by = c("symbol", "date"))
+
+# remove columns with many NA
+keep_cols <- names(which(colMeans(!is.na(dataset)) > 0.5))
+print(paste0("Removing columns with many NA values: ", setdiff(colnames(dataset), c(keep_cols, "right_time"))))
+dataset <- dataset[, .SD, .SDcols = keep_cols]
+
+# remove Inf and Nan values if they exists
+is.infinite.data.frame <- function(x) do.call(cbind, lapply(x, is.infinite))
+keep_cols <- names(which(colMeans(!is.infinite(as.data.frame(dataset))) > 0.98))
+print(paste0("Removing columns with Inf values: ", setdiff(colnames(dataset), keep_cols)))
+dataset <- dataset[, .SD, .SDcols = keep_cols]
+
+# remove inf values
+n_0 <- nrow(dataset)
+clf_data <- dataset[is.finite(rowSums(dataset[, .SD, .SDcols = is.numeric], na.rm = TRUE))]
+n_1 <- nrow(dataset)
+print(paste0("Removing ", n_0 - n_1, " rows because of Inf values"))
+
+
+
+# PREPARE DATA FOR THE TASK -----------------------------------------------
+# change type of year_month_id
+dataset[, year_month_id := as.integer(year_month_id)]
+
+# define predictors
+dataset[, .(date, year_month_id)]
+cols_non_features <- c(
+  "symbol", "date", "date_rolling", "date_ohlcv", "open", "high", "low",
+  "volume", "returns", "year_month_id", "i.close"
+)
+targets <- c(colnames(dataset)[grep("mom_targ", colnames(dataset))])
+cols_features <- setdiff(colnames(dataset), c(cols_non_features, targets))
+
+# remove constant columns in set and remove same columns in test set
+features_ <- dataset[, ..cols_features]
+remove_cols <- colnames(features_)[apply(features_, 2, var, na.rm=TRUE) == 0]
+print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
+cols_features <- setdiff(cols_features, remove_cols)
+
+# convert variables with low number of unique values to factors
+int_numbers = na.omit(dataset[, ..cols_features])[, lapply(.SD, function(x) all(floor(x) == x))]
+int_cols = colnames(dataset[, ..cols_features])[as.matrix(int_numbers)[1,]]
+factor_cols = dataset[, ..int_cols][, lapply(.SD, function(x) length(unique(x)))]
+factor_cols = as.matrix(factor_cols)[1, ]
+factor_cols = factor_cols[factor_cols <= 100]
+dataset = dataset[, (names(factor_cols)) := lapply(.SD, as.factor), .SD = names(factor_cols)]
+
+# remove observations with missing target
+# if we want to keep as much data as possible an use only one predicitn horizont
+# we can skeep this step
+dataset = na.omit(dataset, cols = targets)
+
+# change IDate to date, because of error
+# Assertion on 'feature types' failed: Must be a subset of
+# {'logical','integer','numeric','character','factor','ordered','POSIXct'},
+# but has additional elements {'IDate'}.
+dataset[, date := as.POSIXct(date, tz = "UTC")]
+dataset[, .(symbol,date, date_rolling, year_month_id)]
+
+# sort
+setorder(dataset, date)
+
+# inspect
+dataset[, .(symbol, date)]
+
+
+
+# TASKS -------------------------------------------------------------------
+# id columns we always keep
+id_cols = c("symbol", "date", "year_month_id")
+
+# task with future week returns as target
+target_ = colnames(dataset)[grep("mom_target", colnames(dataset))]
+cols_ = c(id_cols, target_, cols_features)
+task_mom <- as_task_regr(dataset[, ..cols_],
+                         id = "task_mom",
+                         target = target_)
+
+# set roles for symbol, date and yearmonth_id
+task_mom$col_roles$feature = setdiff(task_mom$col_roles$feature, id_cols)
+
+
+
+# CROSS VALIDATIONS -------------------------------------------------------
+# create train, tune and test set
+nested_cv_split = function(task,
+                           train_length = 12,
+                           tune_length = 2,
+                           test_length = 1) {
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom")
+  custom_outer = rsmp("custom")
+
+  # get year month id data
+  # task = task_ret_week$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("year_month_id", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(year_month_id))]
+
+  # util vars
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length)
+  get_row_ids = function(mid) unlist(yearmonthid_[year_month_id %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups <- lapply(start_folds,
+                         function(x) groups_v[x:(x+train_length-1)])
+  train_sets <- lapply(train_groups, get_row_ids)
+
+  # create tune set
+  tune_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+train_length):(x+train_length+tune_length-1)])
+  tune_sets <- lapply(tune_groups, get_row_ids)
+
+  # test train and tune
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_2 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create test sets
+  insample_length = train_length + tune_length
+  test_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_4 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# test
+custom_cvs = nested_cv_split(task_mom)
+custom_inner = custom_cvs$custom_inner
+custom_outer = custom_cvs$custom_outer
+
+# generate cv's
+train_sets = seq(12, 12 * 6, 12 * 2)
+validation_sets = train_sets / 12
+custom_cvs = list()
+for (i in seq_along(train_sets)) {
+  print(i)
+  custom_cvs[[i]] = nested_cv_split(task_mom,
+                                    train_sets[[i]],
+                                    validation_sets[[i]],
+                                    1)
+}
+
+# test if tain , validation and tst set follow logic
+lapply(seq_along(custom_cvs), function(i) {
+  # extract custyom cv
+  custom_cvs_ = custom_cvs[[i]]
+  custom_inner = custom_cvs_$custom_inner
+  custom_outer = custom_cvs_$custom_outer
+
+  # test set start after train set
+  test1 = all(vapply(1:custom_inner$iters, function(i) {
+    (tail(custom_inner$train_set(i), 1) + 1) == custom_inner$test_set(i)[1]
+  }, FUN.VALUE = logical(1L)))
+
+  # train set in outersample contains ids in innersample 1
+  test2  = all(vapply(1:custom_inner$iters, function(i) {
+    all(c(custom_inner$train_set(i),
+          custom_inner$test_set(i)) == custom_outer$train_set(i))
+  }, FUN.VALUE = logical(1L)))
+  c(test1, test2)
+})
+
+
+
+# ADD PIPELINES -----------------------------------------------------------
+# source pipes, filters and other
+source("R/mlr3_winsorization.R")
+source("R/mlr3_uniformization.R")
+source("R/mlr3_gausscov_f1st.R")
+source("R/mlr3_gausscov_f3st.R")
+source("R/mlr3_dropna.R")
+source("R/mlr3_dropnacol.R")
+source("R/mlr3_filter_drop_corr.R")
+source("R/mlr3_winsorizationsimple.R")
+source("R/mlr3_winsorizationsimplegroup.R")
+source("R/PipeOpPCAExplained.R")
+# measures
+source("R/Linex.R")
+source("R/PortfolioRet.R")
+source("R/AdjLoss2.R")
+
+# add my pipes to mlr dictionary
+mlr_pipeops$add("uniformization", PipeOpUniform)
+mlr_pipeops$add("winsorize", PipeOpWinsorize)
+mlr_pipeops$add("winsorizesimple", PipeOpWinsorizeSimple)
+mlr_pipeops$add("winsorizesimplegroup", PipeOpWinsorizeSimpleGroup)
+mlr_pipeops$add("dropna", PipeOpDropNA)
+mlr_pipeops$add("dropnacol", PipeOpDropNACol)
+mlr_pipeops$add("dropcorr", PipeOpDropCorr)
+mlr_pipeops$add("pca_explained", PipeOpPCAExplained)
+mlr_filters$add("gausscov_f1st", FilterGausscovF1st)
+mlr_filters$add("gausscov_f3st", FilterGausscovF3st)
+mlr_measures$add("linex", Linex)
+mlr_measures$add("portfolio_ret", PortfolioRet)
+mlr_measures$add("adjloss2", AdjLoss2)
+
+
+
+# GRAPH -------------------------------------------------------------------
+graph_template =
+  # po("subsample") %>>% # uncomment this for hyperparameter tuning
+  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+  po("dropna", id = "dropna") %>>%
+  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+  po("fixfactors", id = "fixfactors") %>>%
+  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+  po("winsorizesimplegroup", group_var = "year_month_id", id = "winsorizesimplegroup", probs_low = 0.001, probs_high = 0.999, na.rm = TRUE) %>>%
+  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+  po("uniformization") %>>%
+  po("dropna", id = "dropna_v2") %>>%
+  po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0) %>>%
+  po("branch", options = c("nop_filter", "modelmatrix"), id = "interaction_branch") %>>%
+  gunion(list(
+    po("nop", id = "nop_filter"),
+    po("modelmatrix", formula = ~ . ^ 2))) %>>%
+  po("unbranch", id = "interaction_unbranch") %>>%
+  po("removeconstants", id = "removeconstants_3", ratio = 0)
+
+# hyperparameters template
+search_space_template = ps(
+  # subsample for hyperband
+  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  interaction_branch.selection = p_fct(levels = c("nop_filter", "modelmatrix"))
+)
+
+# random forest graph
+graph_rf = graph_template %>>%
+  po("learner", learner = lrn("regr.ranger"))
+graph_rf = as_learner(graph_rf)
+as.data.table(graph_rf$param_set)[, .(id, class, lower, upper, levels)]
+search_space_rf = search_space_template$clone()
+search_space_rf$add(
+  ps(regr.ranger.max.depth = p_int(1, 40))
+)
+
+# xgboost graph
+graph_xgboost = graph_template %>>%
+  po("learner", learner = lrn("regr.xgboost"))
+graph_xgboost = as_learner(graph_xgboost)
+as.data.table(graph_xgboost$param_set)[grep("alpha", id), .(id, class, lower, upper, levels)]
+search_space_xgboost = ps(
+  # subsample for hyperband
+  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  interaction_branch.selection = p_fct(levels = c("nop_filter", "modelmatrix")),
+  # learner
+  regr.xgboost.alpha = p_dbl(0.001, 100, logscale = TRUE)
+)
+
+# inspect search space
+design = rbindlist(generate_design_grid(search_space_xgboost, 3)$transpose(), fill = TRUE)
+design
+
+
+
+# NESTED CV BENCHMARK -----------------------------------------------------
+# nested for loop
+# plan("multises(sion", workers = 2L)
+lapply(custom_cvs, function(cv_) {
+  # for (cv_ in custom_cvs) {
+
+  # debug
+  # cv_ = custom_cvs[[1]]
+
+  # get cv inner object
+  cv_inner = cv_$custom_inner
+  cv_outer = cv_$custom_outer
+  cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
+
+  for (i in 1:custom_inner$iters) {
+    # debug
+    # i = 1
+    print(i)
+
+    # inner resampling
+    custom_ = rsmp("custom")
+    custom_$instantiate(task_mom,
+                        list(cv_inner$train_set(i)),
+                        list(cv_inner$test_set(i)))
+
+    # auto tuner rf
+    at_rf = auto_tuner(
+      tuner = tnr("mbo"), # tnr("hyperband", eta = 5),
+      learner = graph_rf,
+      resampling = custom_,
+      measure = msr("adjloss2"),
+      search_space = search_space_rf,
+      term_evals = 10
+      # terminator = trm("none")
+    )
+
+    # auto tuner rf
+    at_xgboost = auto_tuner(
+      tuner = tnr("mbo"), # tnr("hyperband", eta = 5),
+      learner = graph_xgboost,
+      resampling = custom_,
+      measure = msr("adjloss2"),
+      search_space = search_space_xgboost,
+      term_evals = 10
+      # terminator = trm("none")
+    )
+
+    # outer resampling
+    customo_ = rsmp("custom")
+    customo_$instantiate(task_mom, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
+
+    # nested CV for one round
+    design = benchmark_grid(
+      tasks = list(task_mom),
+      learners = list(at_rf, at_xgboost),
+      resamplings = customo_
+    )
+    bmr = benchmark(design, store_models = TRUE)
+
+    # save locally and to list
+    time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+    file_name = paste0("cv-", cv_$custom_inner$iters, "-", i, "-", time_, ".rds")
+    saveRDS(bmr, file.path(MLR3SAVEPATH, file_name))
+  }
+})
+
+
+
+# test
+dt[symbol == "gef" & date %between% c("2021-09-01", "2021-09-03"), .(symbol, date, open, open_adj, close, close_adj)]
+
+
diff --git a/R/strategy_momentum_ml_v4.R b/R/strategy_momentum_ml_v4.R
new file mode 100644
index 0000000..d5f49fb
--- /dev/null
+++ b/R/strategy_momentum_ml_v4.R
@@ -0,0 +1,177 @@
+library(data.table)
+library(lubridate)
+library(QuantTools)
+library(mlr3verse)
+
+
+
+# SET UP ------------------------------------------------------------------
+# globals
+DATAPATH     = "F:/lean_root/data/all_stocks_daily.csv"
+
+
+# IMPORT DATA -------------------------------------------------------------
+# import QC daily data
+col = c("date", "open", "high", "low", "close", "volume", "close_adj", "symbol")
+dt = fread(DATAPATH, col.names = col)
+dt <- unique(dt, by = c("symbol", "date"))
+unadjustd_cols = c("open", "high", "low")
+adjusted_cols = paste0(unadjustd_cols, "_adj")
+dt[, (adjusted_cols) := lapply(.SD, function(x) (close_adj / close) * x), .SDcols = unadjustd_cols]
+dt = na.omit(dt)
+setorder(dt, symbol, date)
+
+# free resources
+gc()
+
+
+# PREDICTORS --------------------------------------------------------------
+# create help colummns
+dt[, year_month_id := ceiling_date(date, unit = "month") - days(1)]
+
+# remove negative prices
+dt = dt[close > 0 & close_adj > 0]
+
+# add variables
+dt[, dollar_volume := volume * close]
+
+# PRA window sizes
+windows_ = c(5, 22, 22 * 3, 22 * 6, 252, 252 * 2, 252 * 4)
+
+# calculate PRA predictors
+pra_predictors = paste0("pra_", windows_)
+dt[, (pra_predictors) := lapply(windows_,
+                                function(w) roll_percent_rank(close_adj, w)),
+   by = symbol]
+
+# calculate momentum indicators
+setorder(dt, symbol, date)
+moms = seq(21, 21 * 12, 21)
+mom_cols = paste0("mom", 1:12)
+dt[, (mom_cols) := lapply(moms, function(x) close_adj  / shift(close_adj , x) - 1), by = symbol]
+moms = seq(21 * 2, 21 * 12, 21)
+mom_cols_lag = paste0("mom_lag", 2:12)
+dt[, (mom_cols_lag) := lapply(moms, function(x) shift(close_adj, 21) / shift(close_adj , x) - 1), by = symbol]
+
+# create target var
+moms_target = c(5, 10, 21, 42, 63)
+moms_target_cols = paste0("mom_target_", c("w", "2w", "m", "2m", "3m"))
+dt[, (moms_target_cols) := lapply(moms_target, function(x) (shift(close_adj , -x, "shift") / close_adj ) - 1),
+   by = symbol]
+
+# order data
+setorder(dt, symbol, date)
+
+
+# TASKS --------------------------------------------------------------------
+# convert date to PosixCt because it is requireed by mlr3
+dt[, date := as.POSIXct(date, tz = "UTC")]
+dt[, year_month_id := as.numeric(year_month_id)]
+dt[, dollar_volume := as.numeric(dollar_volume)]
+dt[, volume := as.numeric(volume)]
+
+# help to choose columns
+id_cols = c("symbol", "date", "year_month_id")
+cols_features = colnames(dt)[grep("mom\\d+|mom_lag|dollar|close|pra", colnames(dt))]
+print(colnames(dt)[grep("^mom_target", colnames(dt))])
+
+# task with future week returns as target
+print(colnames(dt)[grep("^mom_target", colnames(dt))])
+target_ = "mom_target_m"
+cols_ = c(id_cols, target_, cols_features)
+task = na.omit(dt, cols = cols_)
+task <- as_task_regr(task,
+                     id = "task",
+                     target = target_)
+
+# set roles for symbol, date and yearmonth_id
+task$col_roles$feature = setdiff(task$col_roles$feature, id_cols)
+
+
+# CROSS VALIDATIONS -------------------------------------------------------
+print("Cross validations")
+
+# create train, tune and test set
+nested_cv_split = function(task,
+                           train_length = 12,
+                           tune_length = 1,
+                           test_length = 1) {
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom")
+  custom_outer = rsmp("custom")
+
+  # get year month id data
+  # task = task_ret_week$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+
+  # util vars
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length)
+  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups <- lapply(start_folds,
+                         function(x) groups_v[x:(x+train_length-1)])
+  train_sets <- lapply(train_groups, get_row_ids)
+
+  # create tune set
+  tune_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+train_length):(x+train_length+tune_length-1)])
+  tune_sets <- lapply(tune_groups, get_row_ids)
+
+  # test train and tune
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_2 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create test sets
+  insample_length = train_length + tune_length
+  test_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_4 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# generate cv's
+train_sets = seq(12, 12 * 3, 12)
+validation_sets = train_sets / 12
+custom_cvs = list()
+for (i in seq_along(train_sets)) {
+  print(i)
+  custom_cvs[[i]] = nested_cv_split(task_ret_week,
+                                    train_sets[[i]],
+                                    validation_sets[[i]],
+                                    1)
+}
diff --git a/R/strategy_momentum_wfo.R b/R/strategy_momentum_wfo.R
new file mode 100644
index 0000000..3ac4df9
--- /dev/null
+++ b/R/strategy_momentum_wfo.R
@@ -0,0 +1,231 @@
+library(data.table)
+library(lubridate)
+library(PerformanceAnalytics)
+library(ggplot2)
+library(future.apply)
+
+
+
+
+# UTILS -------------------------------------------------------------------
+# strategy performance
+Performance <- function(x) {
+  cumRetx = Return.cumulative(x)
+  annRetx = Return.annualized(x, scale=252)
+  sharpex = SharpeRatio.annualized(x, scale=252)
+  winpctx = length(x[x > 0])/length(x[x != 0])
+  annSDx = sd.annualized(x, scale=252)
+
+  DDs <- findDrawdowns(x)
+  maxDDx = min(DDs$return)
+  maxLx = max(DDs$length)
+
+  Perf = c(cumRetx, annRetx, sharpex, winpctx, annSDx, maxDDx, maxLx)
+  names(Perf) = c("Cumulative Return", "Annual Return","Annualized Sharpe Ratio",
+                  "Win %", "Annualized Volatility", "Maximum Drawdown", "Max Length Drawdown")
+  return(Perf)
+}
+
+
+
+# import adjusted daily market data
+dt = fread("F:/lean_root/data/all_stocks_daily.csv")
+
+# this want be necessary after update
+setnames(dt, c("date", "open", "high", "low", "close", "volume", "close_adj", "symbol"))
+
+# select columns
+dt = dt[, .(symbol, date, close, close_adj, volume)]
+
+# remove duplicates
+dt = unique(dt, by = c("symbol", "date"))
+
+# remove missing values
+dt = na.omit(dt)
+
+# create help colummns
+dt[, year_month_id := ceiling_date(date, unit = "month") - days(1)]
+
+# remove negative prices
+dt = dt[close > 0 & close_adj > 0]
+
+# add variables
+dt[, dollar_volume := volume * close]
+
+# downsample to lower frequency
+# dtm = dt[, .(date = tail(date, 1),
+#              close = tail(close_adj, 1),
+#              close_raw = tail(close, 1),
+#              volume_mean = mean(volume, na.rm = TRUE),
+#              dollar_volume = sum(dollar_volume, na.rm = TRUE),
+#              dollar_volume_mean = mean(dollar_volume, na.rm = TRUE),
+#              volume = sum(volume, na.rm = TRUE)),
+#          by = c("symbol", "year_month_id")]
+# setorder(dtm, symbol, year_month_id)
+
+# calculate momentum indicators
+setorder(dt, symbol, date)
+moms = seq(21, 21 * 12, 21)
+mom_cols = paste0("mom", 1:12)
+dt[, (mom_cols) := lapply(moms, function(x) close_adj  / shift(close_adj , x) - 1), by = symbol]
+moms = seq(21 * 2, 21 * 12, 21)
+mom_cols_lag = paste0("mom_lag_", 2:12)
+dt[, (mom_cols_lag) := lapply(moms, function(x) shift(close_adj, 21) / shift(close_adj, x) - 1), by = symbol]
+
+# create target var
+moms_target = c(5, 10, 21, 42, 63)
+moms_target_cols = paste0("mom_target_", c("w", "2w", "m", "2m", "3m"))
+dt[, (moms_target_cols) := lapply(moms_target, function(x) (shift(close_adj , -x, "shift") / close_adj ) - 1),
+   by = symbol]
+
+# downsample
+dtm = dt[, .SD[.N], by = c("symbol", "year_month_id")]
+dtm_2 = dt[, .(
+  dollar_volume_sum = sum(dollar_volume, na.rm = TRUE),
+  dollar_volume_mean = mean(dollar_volume, na.rm = TRUE),
+  volume_sum = sum(volume, na.rm = TRUE),
+  volume_mean = mean(volume, na.rm = TRUE)
+), by = c("symbol", "year_month_id")]
+dtm = dtm_2[dtm, on = c("symbol", "year_month_id")]
+dtm[symbol == "aapl"]
+
+# order data
+setorder(dtm, symbol, year_month_id)
+
+# plots
+plot(dtm[symbol == "aapl", .(date, dollar_volume)],
+     type = "l", main = "Volume and dolalr volme")
+plot(dtm[symbol == "aapl", .(date, close_adj)],
+     type = "l", main = "Volume and dolalr volme")
+
+
+
+
+# BACKTEST OPTIMIZATION ---------------------------------------------------
+# parameters
+num_coarse = c(50, 100, 200, 300, 500, 1000, 2000) # number of stocks we leave after coarse universe
+filter_var = "dollar_volume_mean"                  # variable we use for filtering in coarse universe
+min_price = c(1, 2, 5, 10)                         # minimal unadjusted price of the stock at month
+num_long = c(10, 30, 50)                         # number of stocks in portfolio
+mom_vars = colnames(dt)[grep("mom\\d+$|lag", colnames(dt))] # momenutm variables we apply ranking on
+target_vars = "mom_target_m" # colnames(dt)[grep("target", colnames(dt))] # target variables
+risk_variable = c("pra", "minmax")
+params = expand.grid(num_coarse, filter_var, min_price, num_long, mom_vars,
+                     target_vars,
+                     stringsAsFactors = FALSE)
+colnames(params) = c("num_coarse", "filter_var", "min_price", "num_long",
+                     "mom_vars", "target_variables")
+
+# momentum backtest function
+strategy_momentum = function(x,
+                             filter_var = "dollar_volume_mean",
+                             num_coarse = 200,
+                             min_price = 1,
+                             num_long = 50,
+                             mom_var = "mom11",
+                             target_variables = "mom_target_m",
+                             return_cumulative = TRUE) {
+
+  # debug
+  # num_coarse = 50
+  # min_price = 2
+  # num_long = 10
+  # mom_var = "mom10"
+  # target_variables = "mom_target_m"
+  # x = copy(dtm)
+
+  # remove missing values
+  cols = c("symbol", "year_month_id", "close", filter_var, mom_var, target_variables)
+  x = na.omit(x[, ..cols])
+
+  # remove assets with price < x$
+  x = x[, .SD[close > min_price], by = year_month_id]
+
+  # filter 500 with highest volume
+  setorderv(x, c("year_month_id", filter_var), order = 1L)
+  x = x[, tail(.SD, num_coarse), by = year_month_id]
+
+  # choose n with highest growth
+  setorderv(x, c("year_month_id", mom_var), order = c(1, -1))
+  y = x[, head(.SD, num_long), by = year_month_id]
+  # y[year_month_id == as.Date("2023-04-30")]
+
+  # get returns by month
+  results = y[, .(returns = sum(get(target_variables) * 1/nrow(.SD))), by = year_month_id]
+
+  # CAR
+  if (return_cumulative) {
+    perf = Performance(as.xts.data.table(results))
+    perf = as.data.table(t(perf))
+    return(perf)
+  } else {
+    return(results)
+  }
+}
+strategy_momentum(dtm, "dollar_volume_mean", 200, 1, 50, "mom11", "mom_target_m") # example
+
+# backtest across all parameters
+start_time = Sys.time()
+results_l = lapply(1:nrow(params), function(i) {
+  strategy_momentum(dtm,
+                    filter_var = params$filter_var[i],
+                    num_coarse = params$num_coarse[i],
+                    min_price = params$min_price[i],
+                    num_long = params$num_long[i],
+                    mom_var = params$mom_vars[i],
+                    target_variables = params$target_variables[i])
+})
+end_time = Sys.time()
+end_time - start_time
+
+# merge params and results_l
+
+# analyse results
+tail(results[order(`Annualized Sharpe Ratio`), ], 10)
+tail(results[order(`Maximum Drawdown`), ], 10)
+results[mom_vars == "mom10" & min_price == 2 & num_long == 10 & num_coarse == 50]
+ggplot(results, aes(num_coarse, min_price, fill = results_l)) +
+  geom_tile()
+ggplot(results, aes(num_coarse, num_long, fill = results_l)) +
+  geom_tile()
+ggplot(results, aes(min_price, num_long, fill = results_l)) +
+  geom_tile()
+
+# inspect atom
+strategy_momentum(dtm, "dollar_volume_mean", 50, 2, 10, "mom_lag10", "mom_target_m", TRUE)
+x = strategy_momentum(dtm, "dollar_volume_mean", 50, 2, 10, "mom_lag10", "mom_target_m", FALSE)
+charts.PerformanceSummary(x)
+x[order(returns)]
+
+# compare with QC
+dtm[year_month_id == "2023-05-31"] # same
+symbol_ = "isee"
+dt[symbol == symbol_]
+plot(as.xts.data.table(dt[symbol == symbol_, .(date, close_adj)]), type = "l")
+dtm[symbol == symbol_]
+dtm[symbol == symbol_ & date == "2023-04-28"]
+dtms[symbol == symbol_]
+dtms[symbol == symbol_ & date == "2010-01-31"]
+# Symbol F close_t-n 1.550195415 close_t-1 6.373025595 time 2010-01-01 00:00:00
+# Symbol LVS close_t-n 1.72733057 close_t-1 10.91752951 time 2010-01-29 15:00:00   5.320463
+# Symbol LVS close_t-n 1.6673075 close_t-1 10.31062958 time 2010-02-01  12:00:00   5.184
+6.373025595 / 1.550195415 - 1
+dt[symbol == symbol_ & date %between% c("2023-02-01", "2023-04-02")]
+dt[symbol == symbol_ & date %between% c("2009-01-26", "2009-02-02")]
+dt[symbol == symbol_ & date %between% c(as.Date("2009-12-28") - 270, as.Date("2009-12-28") - 230)]
+dt[symbol == symbol_ & round(close_adj, 3) == 1.727]
+dt[symbol == symbol_ & round(close_adj, 3) == 11.058]
+
+
+# vis results
+charts.PerformanceSummary(portfolio_ret)
+
+# Portfolio results
+SharpeRatio(portfolio_ret)
+SharpeRatio.annualized(portfolio_ret)
+
+
+# inspect
+test = as.data.table(portfolio_ret)
+test[portfolio.returns > 0.5]
+test[portfolio.returns <- -0.5]
diff --git a/R/strategy_predictors_daily.R b/R/strategy_predictors_daily.R
new file mode 100644
index 0000000..17a16bb
--- /dev/null
+++ b/R/strategy_predictors_daily.R
@@ -0,0 +1,13 @@
+library(arrow)
+library(fs)
+library(data.table)
+
+
+# import data
+backcusum_files = dir_ls("F:/equity/usa/predictors-daily/backcusum")
+backusum = lapply(backcusum_files, read_parquet)
+names(backusum) = fs::path_ext_remove(fs::path_file(names(backusum)) )
+backusum = rbindlist(backusum, idcol = "symbol")
+
+# free memory
+gc()
diff --git a/R/strategy_rls_mining.R b/R/strategy_rls_mining.R
new file mode 100644
index 0000000..9506dd0
--- /dev/null
+++ b/R/strategy_rls_mining.R
@@ -0,0 +1,141 @@
+library(data.table)
+library(lubridate)
+library(portsort)
+library(finfeatures)
+
+
+
+# import daily data
+dt = fread("F:/lean_root/data/all_stocks_daily.csv")
+
+# this want be necessary after update
+setnames(dt, c("date", "open", "high", "low", "close", "volume", "close_adj", "symbol"))
+
+# remove duplicates
+dt = unique(dt, by = c("symbol", "date"))
+
+# create help colummns
+dt[, year_month_id := ceiling_date(date, unit = "month") - days(1)]
+
+# remove negative prices
+dt = dt[close > 0 & close_adj > 0]
+
+# add variables
+dt[, dollar_volume := volume * close]
+
+# adjust all columns
+dt[, ratio := close_adj / close]
+dt[, `:=`(
+  open_adj = open * ratio,
+  high_adj = high * ratio,
+  low_adj = low * ratio
+)]
+dt[symbol == "aapl"]
+
+# keep only adjusted columns and raw close
+dt = dt[, .(symbol, date, open = open_adj, high = high_adj, low = low_adj,
+            close = close_adj, close_raw = close, volume, dollar_volume, year_month_id)]
+
+# downsample
+dtm = dt[, .(
+  date = tail(date, 1),
+  open = head(open, 1),
+  high = max(high),
+  low = min(low),
+  close = tail(close, 1),
+  volume_mean = mean(volume, na.rm = TRUE),
+  dollar_volume = sum(dollar_volume, na.rm = TRUE),
+  dollar_volume_mean = mean(dollar_volume, na.rm = TRUE),
+  volume = sum(volume, na.rm = TRUE)
+), by = c("symbol", "year_month_id")]
+setorder(dtm, symbol, year_month_id)
+
+# create forward returns
+dtm[, ret_forward := shift(close, -1, type = "shift") / close - 1, by = symbol]
+
+# # predictors
+# ohlcv = Ohlcv$new(dtm, date_col = "year_month_id")
+# OhlcvFeaturesInstance = OhlcvFeatures$new(windows = c(6, 12), quantile_divergence_window = c(6, 12))
+# predictors_ohlcv = predictors_ohlcv$get_ohlcv_features(OhlcvFeaturesInstance)
+#
+# dtm[, mom11 := shift(close, 11, "lag") / close - 1]
+# dtm[, mom6 := shift(close, 6, "lag") / close - 1]
+# predictors = c("mom6", "mom11")
+
+# select cols
+cols_ = c("symbol", "year_month_id", "ret_forward", "dollar_volume_mean", predictors)
+dtm = dtm[, ..cols_]
+dtm = na.omit(dtm)
+
+# remove assets with price < x$
+x = x[, .SD[close > min_price], by = year_month_id]
+
+# filter 500 with highest volume
+filter_var = "dollar_volume_mean" # PARAMETER
+num_coarse = 500                  # PARAMETER
+setorderv(dtm, c("year_month_id", filter_var), order = 1L)
+dtm = dtm[, tail(.SD, num_coarse), by = year_month_id]
+
+# make function that returns results for every predictor
+portsort_results_l = lapply(predictors, function(p) {
+  # debug
+  # p = "mom11"
+  print(p)
+
+  # sample only relevant data
+  cols = c("symbol", "year_month_id", "ret_forward", p)
+  dtm_sample = dtm[, ..cols]
+
+  # predictors matrix
+  Fa = dcast(dtm_sample, year_month_id ~ symbol, value.var = p)
+  setorder(Fa, year_month_id)
+  Fa = as.xts.data.table(Fa)
+  cols_with_na <- apply(Fa, MARGIN = 2, FUN = function(x) sum(is.na(x)) > as.integer(nrow(Fa) * 0.8))
+  Fa = Fa[, !cols_with_na]
+
+  # Forward returns
+  R_forward = as.xts.data.table(dcast(dtm, year_month_id ~ symbol), value.var = "ret_forward")
+  R_forward = R_forward[, !cols_with_na]
+
+  # uncondtitional sorting
+  dimA = 0:10/10
+  sort.output.uncon = unconditional.sort(Fa,
+                                         Fb=NULL,
+                                         Fc=NULL,
+                                         R_forward,
+                                         dimA=dimA,
+                                         dimB=NULL,
+                                         dimC=NULL,
+                                         type = 7)
+  return(sort.output.uncon)
+})
+
+# extract CAR an SR
+portsort_l = lapply(portsort_results_l, function(x) table.AnnualizedReturns(x$returns))
+portsort_l = lapply(portsort_l, function(x) as.data.table(x, keep.rownames = TRUE))
+names(portsort_l) = predictors
+portsort_dt = rbindlist(portsort_l, idcol = "predictor")
+
+as.data.table(portsort_l[[1]], keep.rownames = TRUE)
+
+# # predictors matrix
+# Fa = dcast(dtm[, .(symbol, year_month_id, mom)], year_month_id ~ symbol)
+# setorder(Fa, year_month_id)
+# # Fa = Fa[year_month_id > as.Date("2015-01-01")]
+# Fa = as.xts.data.table(Fa)
+# cols_with_na <- apply(Fa, MARGIN = 2, FUN = function(x) sum(is.na(x)) > as.integer(nrow(Fa) * 0.8))
+# Fa = Fa[, !cols_with_na]
+#
+# # Forward returns
+# R_forward = as.xts.data.table(dcast(dtm[, .(symbol, year_month_id, ret_forward)], year_month_id ~ symbol))
+# R_forward = R_forward[, !cols_with_na]
+#
+# # uncondtitional sorting
+# dimA = 0:10/10
+# sort.output.uncon = unconditional.sort(Fa, Fb=NULL, Fc=NULL,
+#                                        R_forward,
+#                                        dimA=dimA, dimB=NULL, dimC=NULL,
+#                                        type = 7)
+#
+# # Set the scale to 365 (Cryptocurreny markets have no close) and geometric to FALSE (we are using log returns)
+# table.AnnualizedReturns(sort.output.uncon$returns, scale = 12, geometric = TRUE, digits = 3)
diff --git a/R/strategy_rls_mining_v2.R b/R/strategy_rls_mining_v2.R
new file mode 100644
index 0000000..5c7af0e
--- /dev/null
+++ b/R/strategy_rls_mining_v2.R
@@ -0,0 +1,308 @@
+library(data.table)
+library(arrow)
+library(lubridate)
+library(portsort)
+library(PerformanceAnalytics)
+
+
+
+# SETUP -------------------------------------------------------------------
+# Paths
+URIFACTORS = "F:/equity/usa/predictors-daily/factors"
+
+
+# UTILS -------------------------------------------------------------------
+# Function to check if more than 50% of a column's values are zeroes
+check_zero_proportion <- function(column, prop = 0.5) {
+  zero_count <- sum(column == 0, na.rm = TRUE)
+  total_count <- length(column)
+  return(zero_count / total_count > prop)
+}
+
+# MARKET DATA AND FUNDAMENTALS ---------------------------------------------
+# import factors
+fundamentals = read_parquet(file.path(URIFACTORS, "fundamental_factors.parquet"))
+prices = read_parquet(file.path(URIFACTORS, "prices_factors.parquet"))
+macros = read_parquet(file.path(URIFACTORS, "macro_factors.parquet"))
+
+# create month colummn
+prices[, year_month_id := ceiling_date(date, unit = "month") - days(1)]
+colnames(prices)
+
+# add variables
+prices[, dollar_volume := volume * close]
+
+# predictor columns
+predictors_prices = colnames(prices)[c(3:37, 44:99)]
+predictors_prices = setdiff(predictors_prices,
+                            c("sector", "industry", "market_returns",
+                              "dollar_volume"))
+predictors_fund = colnames(fundamentals)
+predictors_fund = setdiff(predictors_fund, c("symbol", "date", "reportedCurrency",
+                                             "fillingDate", "cik", "acceptedDate",
+                                             "calendarYear", "period"))
+predictors = c(predictors_prices, predictors_fund)
+
+# downsample to monthly frequency
+prices_m = prices[, .(
+  date = tail(date, 1),
+  open = head(open, 1),
+  high = max(high),
+  low = min(low),
+  close = tail(close, 1),
+  volume_mean = mean(volume, na.rm = TRUE),
+  dollar_volume = sum(dollar_volume, na.rm = TRUE),
+  dollar_volume_mean = mean(dollar_volume, na.rm = TRUE),
+  volume = sum(volume, na.rm = TRUE)
+), by = c("symbol", "year_month_id")]
+
+# keep lat observation by symbol and date for all predictors
+prices[, .(symbol, date)] # ordered as expected
+prices_predictors = unique(prices, by = c("symbol", "year_month_id"), fromLast = TRUE)
+cols = c("symbol", "year_month_id", predictors_prices)
+prices_predictors = prices_predictors[, ..cols]
+
+# merge predictors by month and prices_m
+proces_m_all = merge(prices_m, prices_predictors, by = c("symbol", "year_month_id"))
+
+# add fundamentals
+proces_m_all[, year_month_id_ := year_month_id]
+fundamentals[, fillingDate_ := fillingDate]
+proces_m_all = fundamentals[proces_m_all, on = c("symbol", "fillingDate_" = "year_month_id_"), roll = Inf]
+proces_m_all[, fillingDate_ := NULL]
+proces_m_all[, .(symbol, date, fillingDate, year_month_id)]
+
+# create forward returns
+proces_m_all[, ret_forward := shift(close, -1, type = "shift") / close - 1, by = symbol]
+proces_m_all[, .(symbol, year_month_id, close, ret_forward)]
+
+# checl for duplicates
+dup_index = which(proces_m_all[, duplicated(.SD[, .(symbol, year_month_id)])])
+proces_m_all = unique(proces_m_all, by = c("symbol", "year_month_id"))
+
+# select cols
+cols_ = c("symbol", "year_month_id", "ret_forward", "dollar_volume_mean", predictors)
+proces_m_all = proces_m_all[, ..cols_]
+# dtm = na.omit(dtm)
+
+# # remove assets with price < x$
+# min_price = 1
+# x = x[, .SD[close > min_price], by = year_month_id]
+
+# filter 500 with highest volume
+filter_var = "dollar_volume_mean" # PARAMETER
+num_coarse = 2000                  # PARAMETER
+setorderv(proces_m_all, c("year_month_id", filter_var), order = 1L)
+proces_m_all_filter = proces_m_all[, tail(.SD, num_coarse), by = year_month_id]
+
+# remove observations with many NA's
+threshold = 0.1
+na_cols <- sapply(proces_m_all_filter, function(x) sum(is.na(x))/length(x) > threshold)
+print(na_cols[na_cols == TRUE])
+print(na_cols[na_cols == FALSE])
+predictors_cleaned = setdiff(predictors, names(na_cols[na_cols == TRUE]))
+cols = c("symbol", "year_month_id", "ret_forward", predictors_cleaned)
+prices_cleaned = proces_m_all[, ..cols]
+
+# remove NA values for target
+prices_cleaned = na.omit(prices_cleaned, cols = "ret_forward")
+
+# winsorize extreme values
+predictors_reduced = intersect(predictors, colnames(prices_cleaned))
+prices_cleaned[, (predictors_reduced) := lapply(.SD, as.numeric), .SDcols = predictors_reduced]
+prices_cleaned[, (predictors_reduced) := lapply(.SD, function(x) DescTools::Winsorize(x, probs = c(0.01, 0.99), na.rm = TRUE)),
+               by = year_month_id, .SDcols = predictors_reduced]
+
+# visualize some vars
+var_ = sample(predictors_cleaned, 1)
+# var_ = "illiquidity"
+date_ = prices_cleaned[, sample(year_month_id, 1)]
+x = prices_cleaned[year_month_id == date_, ..var_]
+hist(unlist(x), main=var_, xlab="dots", ylab=var_)
+
+# make function that returns results for every predictor
+prices_cleaned_sample = prices_cleaned[year_month_id > as.Date("2005-01-01")]
+portsort_results_l = lapply(predictors_cleaned, function(p) {
+  # debug
+  # p = "mom12m_lag"
+  print(p)
+
+  # sample only relevant data
+  cols = c("symbol", "year_month_id", "ret_forward", p)
+  prices_cleaned_sample_ = prices_cleaned_sample[, ..cols]
+
+  # predictors matrix
+  Fa = dcast(prices_cleaned_sample_, year_month_id ~ symbol, value.var = p)
+  setorder(Fa, year_month_id)
+  Fa = as.xts.data.table(Fa)
+  cols_with_na <- apply(Fa, MARGIN = 2, FUN = function(x) sum(is.na(x)) > as.integer(nrow(Fa) * 0.6))
+  # dim(Fa)
+  Fa = Fa[, !cols_with_na]
+
+  # remove all NA values
+  rows_with_na <- apply(Fa, MARGIN = 1, FUN = function(x) sum(is.na(x)) > as.integer(ncol(Fa) * 0.99))
+  Fa = Fa[!rows_with_na, ]
+
+  # remove zero values
+  columns_to_remove <- apply(Fa, 2, check_zero_proportion, 0.2)
+  Fa = Fa[, !columns_to_remove]
+
+  # Forward returns
+  R_forward = as.xts.data.table(dcast(prices_cleaned_sample_, year_month_id ~ symbol, value.var = "ret_forward"))
+  R_forward = R_forward[, !cols_with_na]
+  R_forward = R_forward[!rows_with_na, ]
+  R_forward = R_forward[, !columns_to_remove]
+
+  # uncondtitional sorting
+  dimA = 0:6/6
+  psort_out = tryCatch({
+    sort.output.uncon = unconditional.sort(Fa,
+                                           Fb=NULL,
+                                           Fc=NULL,
+                                           R_forward,
+                                           dimA=dimA,
+                                           dimB=NULL,
+                                           dimC=NULL,
+                                           type = 7)
+
+  }, error = function(e) NULL)
+  if (is.null(psort_out)) {
+    print(paste0("Remove variable ", p))
+    return(NULL)
+  } else {
+    return(sort.output.uncon)
+  }
+  # table.AnnualizedReturns(sort.output.uncon$returns)
+})
+
+# extract CAR an SR
+index_keep = sapply(portsort_results_l, function(x) !is.null(x))
+portsort_l <- portsort_results_l[index_keep]
+portsort_l = lapply(portsort_l, function(x) table.AnnualizedReturns(x$returns))
+portsort_l = lapply(portsort_l, function(x) as.data.table(x, keep.rownames = TRUE))
+names(portsort_l) = predictors_cleaned[index_keep]
+portsort_dt = rbindlist(portsort_l, idcol = "predictor")
+portsort_dt
+
+# check for exact monotinicity
+sr_meta = portsort_dt[rn == "Annualized Sharpe (Rf=0%)"]
+sr_meta[apply(sr_meta[, 3:ncol(portsort_dt)], 1, function(x) any(x > 1))]
+sr = sr_meta[rn == "Annualized Sharpe (Rf=0%)", 3:ncol(sr_meta)]
+apply(sr, 1, function(x) all(x == cummax(x)))
+sr_meta[apply(sr, 1, function(x) all(x == cummax(x)))]
+apply(sr, 1, function(x) all(x == cummin(x)))
+sr_meta[apply(sr, 1, function(x) all(x == cummin(x)))]
+sr_meta[apply(sr, 1, function(x) all(cummax(x) - x < 0.1))]
+sr_meta[apply(sr, 1, function(x) all(x - cummin(x) < 0.1))]
+
+# test
+# labelsA <- seq(1, length(dimA) - 1, 1)
+# t = 1
+# for (t in 1:nrow(Fa)) {
+#   print(t)
+#   breaks = c(quantile(Fa[t, ], probs = dimA, na.rm = TRUE, type = 7))
+# }
+# for (t in 1:nrow(Fa)) {
+#   print(t)
+#   Hold.Fa = t(as.matrix(as.numeric(cut(Fa[t, ],
+#                                        breaks = c(quantile(Fa[t, ], probs = dimA, na.rm = TRUE, type = 7)), labels = labelsA,
+#                                        include.lowest = TRUE))))
+# }
+# dim(Fa[t, ])
+# Fa[t, ][, !is.na(Fa[t, ])]
+# x = Fa[t, ][, !is.na(Fa[t, ])]
+# x[, 1:58]
+
+
+
+# PORTSORT 2 PREDICTORS ---------------------------------------------------
+# make function that returns results for every predictor
+prices_cleaned_sample = prices_cleaned[year_month_id > as.Date("2005-01-01")]
+
+# make 2R predictors space
+predictors_2d = c(sr_meta[apply(sr, 1, function(x) all(cummax(x) - x < 0.1)), "predictor"],
+                  sr_meta[apply(sr, 1, function(x) all(x - cummin(x) < 0.1)), "predictor"])
+predictors_2d = unlist(predictors_2d, use.names = FALSE)
+predictors_2d = t(combn(predictors_2d, 2))
+# predictors_2d = predictors_2d[predictors_2d[, 1] != predictors_2d[, 2]]
+# predictors_cleaned_use_2d = expand.grid(predictors_2d,
+#                                         predictors_2d,
+#                                         stringsAsFactors = FALSE)
+portsort_results_2d_l = lapply(1:nrow(predictors_2d), function(i) {
+  # debug
+  # i = 1
+  print(i)
+
+  # sample only relevant data
+  p1 = predictors_2d[i, 1]
+  # p1 = "purchasesOfInvestments"
+  p2 = predictors_2d[i, 2]
+  # p2 = "cashFlowCoverageRatios"
+  cols = c("symbol", "year_month_id", "ret_forward", p1, p2)
+  prices_cleaned_sample_ = prices_cleaned_sample[, ..cols]
+
+  # predictors matrix
+  Fa = dcast(prices_cleaned_sample_, year_month_id ~ symbol, value.var = p1)
+  Fb = dcast(prices_cleaned_sample_, year_month_id ~ symbol, value.var = p2)
+  setorder(Fa, year_month_id)
+  setorder(Fb, year_month_id)
+  Fa = as.xts.data.table(Fa)
+  Fb = as.xts.data.table(Fb)
+
+  # remove all na values from both
+  cols_with_na_fa <- apply(Fa, MARGIN = 2, FUN = function(x) sum(is.na(x)) > as.integer(nrow(Fa) * 0.6))
+  cols_with_na_fb <- apply(Fb, MARGIN = 2, FUN = function(x) sum(is.na(x)) > as.integer(nrow(Fb) * 0.6))
+  cols_with_na = cols_with_na_fa | cols_with_na_fb
+  Fa = Fa[, !cols_with_na]
+  Fb = Fb[, !cols_with_na]
+
+  # remove all NA values
+  rows_with_na_fa <- apply(Fa, MARGIN = 1, FUN = function(x) sum(is.na(x)) > as.integer(ncol(Fa) * 0.99))
+  rows_with_na_fb <- apply(Fb, MARGIN = 1, FUN = function(x) sum(is.na(x)) > as.integer(ncol(Fb) * 0.99))
+  rows_with_na = rows_with_na_fa | rows_with_na_fb
+  Fa = Fa[!rows_with_na, ]
+  Fb = Fb[!rows_with_na, ]
+
+  # remove zero values
+  columns_to_remove_fa <- apply(Fa, 2, check_zero_proportion, 0.2)
+  columns_to_remove_fb <- apply(Fb, 2, check_zero_proportion, 0.2)
+  columns_to_remove = columns_to_remove_fa | columns_to_remove_fb
+  Fa = Fa[, !columns_to_remove]
+  Fb = Fb[, !columns_to_remove]
+
+  # Forward returns
+  R_forward = as.xts.data.table(dcast(prices_cleaned_sample_, year_month_id ~ symbol, value.var = "ret_forward"))
+  R_forward = R_forward[, !cols_with_na]
+  R_forward = R_forward[!rows_with_na, ]
+  R_forward = R_forward[, !columns_to_remove]
+
+  # uncondtitional sorting
+  dimA = 0:6/6
+  dimB = 0:6/6
+  sort.output.uncon = unconditional.sort(Fa,
+                                         Fb=Fb,
+                                         Fc=NULL,
+                                         R_forward,
+                                         dimA=dimA,
+                                         dimB=dimB,
+                                         dimC=NULL,
+                                         type = 7)
+  # table.AnnualizedReturns(sort.output.uncon$returns)
+  return(sort.output.uncon)
+})
+
+# extract CAR an SR
+portsort_2d_l = lapply(portsort_results_2d_l, function(x) table.AnnualizedReturns(x$returns))
+portsort_2d_l = lapply(portsort_2d_l, function(x) as.data.table(x, keep.rownames = TRUE))
+names_ = paste(predictors_2d[, 1], predictors_2d[, 2], sep =  "_")
+names(portsort_2d_l) = names_
+portsort_2d_dt = rbindlist(portsort_2d_l, idcol = "predictor")
+
+# check for monotinicity
+sr_meta = portsort_2d_dt[rn == "Annualized Sharpe (Rf=0%)"]
+index_best = which(apply(sr_meta[, 3:ncol(portsort_dt)], 1, function(x) any(x > 2)))
+sr_meta[index_best]
+
+#
+portsort_results_2d_l[[36]]
+portsort_results_2d_l[[36]]$portfolios
diff --git a/R/strategy_systematic_jump_diffusion_JLD_2023.R b/R/strategy_systematic_jump_diffusion_JLD_2023.R
new file mode 100644
index 0000000..02d7769
--- /dev/null
+++ b/R/strategy_systematic_jump_diffusion_JLD_2023.R
@@ -0,0 +1,52 @@
+library(data.table)
+library(arrow)
+library(lubridate)
+library(xts)
+library(nanotime)
+
+
+
+# SETUP -------------------------------------------------------------------
+# uris
+URIPRICES = "C:/Users/Mislav/SynologyDrive/equity/usa/minute"
+
+# files
+files = list.files(URIPRICES, full.names = TRUE)
+
+
+# IMPORT DATA -------------------------------------------------------------
+# loop that import minute data, upsample to 5 min and return returns
+lapply(files, function(f) {
+  # debug
+  # f = files[1]
+
+  # import minute data
+  dt = read_parquet(f)
+
+  # change timezone
+  # dt[, date := with_tz(date, tzone = "America/New_York")]
+
+  # keep relevant columns
+  # dt = dt[, .(date, close)]
+
+  dt[, date_nano := as.nanotime(date)]
+  dt5 = dt[, .(
+    open = head(open, 1),
+    high = max(high, na.rm = TRUE),
+    low = min(low, na.rm = TRUE),
+    close = tail(close, 1),
+    volume = sum(volume, na.rm = TRUE)
+  ),
+  by = .(time = nano_ceiling(date_nano, as.nanoduration("00:05:00")))]
+  dt5[, time := as.POSIXct(time, tz = "UTC")]
+
+  dt
+  dt5
+
+  # upsample
+  # dt[, minute5 := cut(date, breaks = "5 min")]
+  xts::to.minutes5(as.xts.data.table(dt))
+
+
+
+})
diff --git a/R/stratgy_interest_rate.R b/R/stratgy_interest_rate.R
new file mode 100644
index 0000000..6c699e2
--- /dev/null
+++ b/R/stratgy_interest_rate.R
@@ -0,0 +1,149 @@
+library(data.table)
+library(AzureStor)
+library(httr)
+library(roll)
+library(ggplot2)
+library(duckdb)
+library(lubridate)
+library(gausscov)
+
+
+
+# SET UP ------------------------------------------------------------------
+# globals
+PATHOHLCV = "F:/lean_root/data/equity/usa/daily"
+NASPATH       = "C:/Users/Mislav/SynologyDrive/trading_data"
+
+# # help vars
+cols_ohlc = c("open", "high", "low", "close")
+cols_ohlcv = c("open", "high", "low", "close", "volume")
+
+# creds for Azure Blob Storage
+ENDPOINT = storage_endpoint(Sys.getenv("BLOB-ENDPOINT-SNP"),
+                            Sys.getenv("BLOB-KEY-SNP"))
+cont = storage_container(ENDPOINT, "indexes")
+
+
+
+# DATA --------------------------------------------------------------------
+# import macro data
+macro_indicators = fread(file.path(NASPATH, "macro_predictors.csv"))
+macro_indicators[, date := as.Date(date)]
+
+# help function to import symbols
+get_symbol = function(symbol) {
+  dt = fread(cmd = paste0("unzip -p ", PATHOHLCV, "/", symbol, ".zip"),
+             col.names = c("date", cols_ohlcv))
+  dt[, date := as.Date(substr(date, 1, 8), "%Y%m%d")]
+  dt[, (cols_ohlc) := lapply(.SD, function(x) x / 10000), .SDcols = cols_ohlc]
+  dt = dt[, .(date, close)]
+  setnames(dt, "close", paste0("close_", symbol))
+  return(dt)
+}
+
+# import ohlcv data for bond, equity and commodities
+treasury = get_symbol("shv")
+bondshort = get_symbol("vgsh") # vgsh, scho, spts
+bond = get_symbol("tlt")
+equity = get_symbol("spy")
+commodity = get_symbol("dbc") # # DBC (GSG, USCI)
+tips = get_symbol("tip") # # DBC (GSG, USCI)
+gold = get_symbol("gld")
+
+# merge  all data for daily frequency
+dt = Reduce(function(x, y) merge(x, y, by = "date", all = TRUE),
+            list(macro_indicators, equity, bond, bondshort, treasury, gold))
+
+# fill missing values
+cols = dt[, colnames(.SD), .SDcols = is.numeric]
+dt = dt[, (cols) := lapply(.SD, nafill, type = "locf"), .SDcols = cols]
+
+
+
+
+# DOWNSAMPLE --------------------------------------------------------------
+# downsample
+dataset = copy(dt)
+dataset[, week := floor_date(date, "month")]
+dataset = dataset[, tail(.SD, 1), by = week]
+
+
+
+
+# MODEL -------------------------------------------------------------------
+# define predictors
+non_pred_cols = c("date", "date_merge_dt", "week", "close", "volume",
+                  "date_merge_tlt", "close_spy", "close_tlt", "close_vgsh",
+                  "close_shv", "close_gld", "y_week")
+predictors = setdiff(colnames(dataset), non_pred_cols)
+
+# create target variable
+dataset[, y_week := shift(GS10_m, -1, type = "shift")]
+
+# remove INF values if exists
+X_cols = c(predictors, "y_week")
+any(vapply(dataset[, ..X_cols], function(x) any(is.infinite(x)), FUN.VALUE = logical(1L)))
+dim(dataset)
+numeric_cols <- names(dt)[sapply(dt, is.numeric | is.integer)]
+dataset[, (numeric_cols) := lapply(.SD, function(x) replace(x, is.infinite(x), NA)), .SDcols = numeric_cols]
+dim(dataset)
+
+# remove NA from predictors or target
+dim(dataset)
+dataset = na.omit(dataset, cols = c(predictors, "y_week"))
+dim(dataset)
+
+# prepare X and y
+X = dataset[, ..predictors]
+X[, y_week:= NULL]
+char_cols = X[, colnames(.SD), .SDcols = is.factor]
+X[, (char_cols) := lapply(.SD, as.integer), .SDcols = char_cols]
+X[, ..char_cols]
+X = na.omit(X)
+# formula <- as.formula(paste(" ~ (", paste(colnames(X), collapse = " + "), ")^2"))
+# X = model.matrix(formula, X)
+X = as.matrix(X)
+y = as.matrix(dataset[, .(y_week)])
+
+# insample feature importance using gausscov
+f1st_res = f1st(y = y, x = X, p0=0.1)
+f1st_res = f1st_res[[1]]
+f1st_res_index = f1st_res[f1st_res[, 1] != 0, , drop = FALSE]
+colnames(X)[f1st_res_index[, 1]]
+lm(y ~ X[, colnames(X)[f1st_res_index[, 1]]])
+f3st_res = f3st(y = y, x = X, m = 3, p = 0.2)
+res_index <- unique(as.integer(f3st_res[[1]][1, ]))[-1]
+res_index  <- res_index [res_index  != 0]
+colnames(X)[res_index]
+
+# visualize most important variables
+important_vars = c("GS10",
+                   "change_in_nonrept_long_all:ret_3")
+X_imp = cbind(date = dataset[, date],
+              close = dataset[, close],
+              X[, important_vars], y)
+X_imp = as.data.table(X_imp)
+X_imp[, date := as.POSIXct(date)]
+X_imp[, signal := ifelse(shift(`change_in_noncomm_spead_all:ret_6`) < 300, 0, 1)]
+X_imp[, signal2 := ifelse(shift(`change_in_nonrept_long_all:ret_3`) < 100, 0, 1)]
+ggplot(X_imp, aes(x = date)) +
+  geom_line(aes(y = `change_in_noncomm_spead_all:ret_6`))
+ggplot(X_imp, aes(x = date)) +
+  geom_line(aes(y = `change_in_nonrept_long_all:ret_3`))
+ggplot(X_imp[1:200], aes(x = date)) +
+  geom_line(aes(y = `change_in_noncomm_spead_all:ret_6`))
+ggplot(X_imp[800:1000], aes(x = date)) +
+  geom_line(aes(y = `change_in_noncomm_spead_all:ret_6`))
+ggplot(X_imp[1000:nrow(X_imp)], aes(x = date)) +
+  geom_line(aes(y = `change_in_noncomm_spead_all:ret_6`))
+ggplot(na.omit(X_imp), aes(x = date, color = as.factor(signal))) +
+  geom_line(aes(y = close))
+ggplot(na.omit(X_imp[1000:nrow(X_imp)]), aes(x = date, y = close, color = signal)) +
+  geom_line(size = 1)
+ggplot(na.omit(X_imp[800:1000]), aes(x = date, y = close, color = signal)) +
+  geom_line(size = 1)
+ggplot(na.omit(X_imp[1000:nrow(X_imp)]), aes(x = date, y = close, color = signal2)) +
+  geom_line(size = 1)
+ggplot(na.omit(X_imp[800:1000]), aes(x = date, y = close, color = signal2)) +
+  geom_line(size = 1)
+
diff --git a/R/supek.R b/R/supek.R
new file mode 100644
index 0000000..4ffdee8
--- /dev/null
+++ b/R/supek.R
@@ -0,0 +1,25 @@
+library(ssh)
+
+
+# params
+host = "jarneric@login-cpu.hpc.srce.hr"
+key_path = "C:/Users/Mislav/.ssh/id_rsa_srce"
+paraphrase = "Joskosrce"
+
+session <- ssh_connect(host, keyfile = key_path)
+print(session)
+
+ssh_key_info()
+# execute command
+out <- ssh_exec_wait(session, command = 'whoami')
+out <- ssh_exec_wait(session, command = 'sudo apt install r-base -S Joskosrce')
+out <- ssh_exec_wait(session, command = 'apptainer pull docker://r-base')
+print(out)
+
+
+
+library(data.table)
+url = "https://snpmarketdata.blob.core.windows.net/qc1000/"
+dates = seq.Date(as.Date("1998-01-01"), Sys.Date(), 1)
+url_files = paste0(url, dates, ".csv")
+qc1000 = lapply(url_files, function(x) tryCatch(fread(x), error = function(e) NULL))
diff --git a/R/universe.R b/R/universe.R
index 797f35c..8ad72a8 100644
--- a/R/universe.R
+++ b/R/universe.R
@@ -1,45 +1,49 @@
-library(AzureStor)
 library(data.table)
-library(pins)
-library(writexl)
-library(forecast)
-library(ggplot2)
-library(mlr3verse)
-library(mlr3cluster)
-library(DescTools)
-library(future.apply)
-library(httr)
-library(roll)
-library(readr)
-library(PerformanceAnalytics)
+library(checkmate)
+library(tiledb)
+library(qlcal)
 library(findata)
+library(ggplot2)
 library(lubridate)
-library(patchwork)
-library(fredr)
-library(gausscov)
-library(runner)
-library(rollRegres)
-
-
-# set up
-endpoint <- storage_endpoint(Sys.getenv("BLOB-ENDPOINT"), key=Sys.getenv("BLOB-KEY"))
-board_fundamentals <- board_azure(
-  container = storage_container(endpoint, "fundamentals"), # HERE CHANGE WHEN MINUTE DATA !!!
-  path = "",
-  n_processes = 10,
-  versioned = FALSE,
-  cache = NULL
-)
-CACHEDIR = "D:/findata" # here define your local folder wher data will be saved
-board_prices <- board_azure(
-  container = storage_container(endpoint, "fmpcloud-daily"),
-  path = "",
-  n_processes = 6L,
-  versioned = FALSE,
-  cache = CACHEDIR
-)
-
-# utils
+library(AzureStor)
+
+# library(forecast)
+
+# library(mlr3verse)
+# library(mlr3cluster)
+# library(DescTools)
+# library(future.apply)
+# library(httr)
+# library(roll)
+# library(PerformanceAnalytics)
+
+
+# library(patchwork)
+# library(gausscov)
+# library(runner)
+# library(rollRegres)
+
+
+# SET UP ------------------------------------------------------------------
+# check if we have all necessary env variables
+assert_choice("AWS-ACCESS-KEY", names(Sys.getenv()))
+assert_choice("AWS-SECRET-KEY", names(Sys.getenv()))
+assert_choice("AWS-REGION", names(Sys.getenv()))
+assert_choice("BLOB-ENDPOINT", names(Sys.getenv()))
+assert_choice("BLOB-KEY", names(Sys.getenv()))
+assert_choice("APIKEY-FMPCLOUD", names(Sys.getenv()))
+assert_choice("FRED-KEY", names(Sys.getenv()))
+
+# set credentials
+config <- tiledb_config()
+config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
+config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
+config["vfs.s3.region"] <- Sys.getenv("AWS-REGION")
+context_with_config <- tiledb_ctx(config)
+
+# set calendar
+qlcal::setCalendar("UnitedStates/NYSE")
+
 # date segments
 DOTCOM <- c("2000-01-01", "2002-01-01")
 GFC <- c("2007-01-01", "2010-01-01")
@@ -47,62 +51,183 @@ AFTERGFCBULL <- c("2010-01-01", "2015-01-01")
 COVID <- c("2020-01-01", "2021-06-01")
 AFTER_COVID <- c("2021-06-01", "2022-01-01")
 NEW <- c("2022-01-01", "2022-03-15")
-fred_api_key <- "fb7e8cbac4b84762980f507906176c3c"
-fredr_set_key(fred_api_key)
-
-
-# IMPORT DATA -------------------------------------------------------------
-# market data
-daily_prices_files <- pin_list(board_prices)
-files_new <- setdiff(daily_prices_files, list.files(CACHEDIR))
-lapply(files_new, pin_download, board = board_prices)
-files_local <- vapply(file.path(CACHEDIR, daily_prices_files),
-                      list.files, recursive = TRUE, pattern = "\\.csv", full.names = TRUE,
-                      FUN.VALUE = character(1))
-# plan(multicore(workers = 4L))
-prices_l <- lapply(files_local, fread)
-prices <- prices_l[vapply(prices_l, function(x) nrow(x) > 0, FUN.VALUE = logical(1))]
-prices <- rbindlist(prices)
-
-# clean daily prices
-prices <- prices[open > 0 & high > 0 & low > 0 & close > 0 & adjClose > 0] # remove rows with zero and negative prices
-prices <- unique(prices, by = c("symbol", "date")) #TODO SEND MAIL TO FMP CLOUD ! WHY WE HAVE DUPLICATED ROWS, DIFFERENT ADJUSTED CLOSE PRICES
-setorder(prices, "symbol", "date")
-prices[, returns := adjClose   / data.table::shift(adjClose) - 1, by = symbol] # calculate returns
-adjust_cols <- c("open", "high", "low")
-prices[, (adjust_cols) := lapply(.SD, function(x) x * (adjClose / close)), .SDcols = adjust_cols] # adjust open, high and low prices
-prices[, close_raw := close]
-prices[, close := adjClose]
-prices <- na.omit(prices[, .(symbol, date, close_raw, open, high, low, close, volume, returns)])
-prices <- unique(prices, by = c("symbol", "date")) # remove duplicates if they exists
-
-# TODO: simfin+daily data
-
-# download only USA stocks
-url <- modify_url("https://financialmodelingprep.com/", path = "api/v3/available-traded/list",
-                  query = list(apikey = Sys.getenv("APIKEY-FMPCLOUD") ))
-stocks <- rbindlist(content(GET(url)))
-usa_symbols <- stocks[exchangeShortName %in% c("AMEX", "NASDAQ", "NYSE", "OTC")]
-
-# remove ETF's
-usa_symbols <- usa_symbols[!grep("etf|spdr", name, ignore.case = TRUE)]
+
+
+
+# DATA --------------------------------------------------------------------
+# import market data and fundamentals
+factors = Factors$new()
+factors_l = factors$get_factors(first_date = as.Date("2010-01-01"))
+price_factors <- factors_l$prices_factos
+fundamental_factors <- factors_l$fundamental_factors
+macro <- factors_l$macro
+
+# free resources
+rm(factors_l)
+gc()
+
+# filter dates and symbols
+price_factors_dt <- unique(price_factors, by = c("symbol", "date"))
+
+# change date to data.table date
+price_factors_dt[, date := data.table::as.IDate(date)]
+
+# keep only NYSE trading days
+trading_days <- getBusinessDays(price_factors_dt[, min(date)],
+                                price_factors_dt[, max(date)])
+setkey(price_factors_dt, date)
+price_factors_dt <- price_factors_dt[.(as.IDate(trading_days))]
+setkey(price_factors_dt, NULL)
+
+# order data
+setorder(price_factors_dt, "symbol", "date")
+
+# remove symbols with less than 2 years of data
+prices_n <- price_factors_dt[, .N, by = symbol]
+prices_n <- prices_n[which(prices_n$N > 252*2)]  # remove prices with only 700 or less observations
+price_factors_dt <- price_factors_dt[symbol %in% prices_n$symbol]
+
+# remove missing values
+price_factors_dt <- na.omit(price_factors_dt,
+                            cols = c("symbol", "date", "open", "high", "low",
+                                     "close", "volume", "returns"))
+
+# remove zero and negative prices
+price_factors_dt <- price_factors_dt[open > 0 & high > 0 & low > 0 & close > 0]
+
+# add help columns
+price_factors_dt[, `:=`(year = year(date), month = month(date))] # help columns
+
+# save SPY for later and keep only events symbols
+spy <- price_factors_dt[symbol == "SPY"]
+setorder(spy, date)
+
+
+
+# COARSE UNIVERSE SELECTION ------------------------------------------------------
+# keep most liquid
+n_most_liquid <- 500
+price_factors_dt[, volume_sum := sum(volume, na.rm = TRUE), by = .(symbol, year, month)]
+most_liquid <- unique(price_factors_dt[, .(symbol, year, month, volume_sum)])
+most_liquid <- most_liquid[, .SD[volume_sum %in% tail(sort(unique(volume_sum)), n_most_liquid)], by = .(year, month)]
+prices_most_liquid <- most_liquid[, `:=`(index = 1, volume_sum = NULL)][price_factors_dt, on = .(year = year, month = month, symbol = symbol)]
+prices_most_liquid <- prices_most_liquid[index == 1]
+prices_most_liquid[, index := NULL]
+setorderv(prices_most_liquid, c("year", "month", "symbol"))
+
+# rolling vol
+prices_most_liquid[, vol_monthly := roll::roll_sd(returns, length(returns), min_obs = 1L), by = symbol]
+prices_most_liquid[, vol_monthly := data.table::last(vol_monthly, 1L), by = .(symbol, year, month)]
+
+# keep least volatility
+n_least_volatile <- 250
+least_volatile <- unique(prices_most_liquid[, .(symbol, year, month, vol_monthly)])
+least_volatile <- least_volatile[, .SD[vol_monthly %in% head(sort(unique(vol_monthly)), n_least_volatile)], by = .(year, month)]
+prices_most_liquid_least_vol <- least_volatile[, `:=`(index = 1, vol_monthly = NULL)][
+  prices_most_liquid, on = .(year = year, month = month, symbol = symbol)
+]
+prices_most_liquid_least_vol <- prices_most_liquid_least_vol[index == 1]
+prices_most_liquid_least_vol[, index := NULL]
+setorderv(prices_most_liquid_least_vol, c("year", "month", "symbol"))
+
+
+# FINE FUNDAMENTAL FILTERING  -------------------------------------------------------------
+# choose fundamnetal features
+features_fund <- fundamental_factors[, .(symbol, date, fillingDate, fiveYRevenueGrowthPerShare,
+                                   fiveYDividendperShareGrowthPerShare, payoutRatio,
+                                   returnOnCapitalEmployed, freeCashFlowYield)]
+features_fund[, filling_date := fillingDate]
+setnames(features_fund, "date", "date_fund")
+features_fund <- features_fund[, payoutratio_5y_mean := frollmean(payoutRatio, n = 4 * 5, na.rm = TRUE), by = "symbol"]
+features_fund <- features_fund[, returnOnCapitalEmployed_5y_mean := frollmean(returnOnCapitalEmployed, n = 4 * 5, na.rm = TRUE), by = "symbol"]
+features_fund <- features_fund[, freeCashFlowYield_5y_mean := frollmean(freeCashFlowYield, n = 4 * 5, na.rm = TRUE), by = "symbol"]
+# features <- features[, earningsYield_10Y_mean := frollmean(earningsYield, n = 4 * 10, na.rm = TRUE), by = "symbol"]
+features_fund[, `:=`(payoutRatio.x = NULL, returnOnCapitalEmployed = NULL, freeCashFlowYield = NULL)]
+features_fund[symbol == "MSFT" & date_fund == as.Date("2021-12-31")]
+features_fund[date_fund == as.Date("2021-09-30")]
+
+# create dataset
+DT <- features_fund[prices_most_liquid_least_vol, on = .(symbol = symbol, fillingDate = date), roll = +Inf]
+DT[fillingDate == as.Date("2021-09-30")]
+DT <- na.omit(DT, cols = c("fiveYRevenueGrowthPerShare", "fiveYDividendperShareGrowthPerShare", "freeCashFlowYield_5y_mean"))
+DT[fillingDate == as.Date("2021-09-30")]
+
+# 1) remove stocks with fundamnetals below thresholds
+DT_sample <- DT[fiveYRevenueGrowthPerShare >= 0.05]
+DT_sample <- DT_sample[fiveYDividendperShareGrowthPerShare >= 0.02]
+DT_sample <- DT_sample[payoutratio_5y_mean <= 0.7]
+DT_sample <- DT_sample[returnOnCapitalEmployed_5y_mean >= 0.025]
+DT_sample <- DT_sample[freeCashFlowYield_5y_mean >= 0.01]
+
+# 2) cluster analysis
+data_ <- DT[fillingDate == as.Date("2021-12-31"), .(symbol, fillingDate, fiveYRevenueGrowthPerShare,
+                                                    fiveYDividendperShareGrowthPerShare, freeCashFlowYield_5y_mean)]
+cols <- colnames(data_)[3:ncol(data_)]
+summary(data_[, ..cols])
+data_[, (cols) := lapply(.SD, function(x) {Winsorize(as.numeric(x), probs = c(0.05, 0.95), na.rm = TRUE)}), .SDcols = cols]
+summary(data_[, ..cols])
+data_task <- TaskClust$new(id = "fundamentals", backend = data_[, 3:ncol(data_)])
+design = benchmark_grid(
+  tasks = data_task,
+  learners = list(
+    lrn("clust.kmeans", centers = 3L),
+    lrn("clust.pam", k = 3L),
+    lrn("clust.cmeans", centers = 3L),
+    lrn("clust.ap"),
+    # lrn("clust.SimpleKMeans"),
+    lrn("clust.agnes", k = 3L)),
+  resamplings = rsmp("insample"))
+print(design)
+
+# execute benchmark
+bmr = benchmark(design)
+
+# define measure
+measures = list(msr("clust.wss"), msr("clust.silhouette"))
+bmr$aggregate(measures)
+
+# predictions
+task = TaskClust$new(id = "fundamentals_pred", backend = data_[, 3:ncol(data_)])
+learner = lrn("clust.pam", k = 3L)
+learner$train(task)
+preds = learner$predict(task = task)
+autoplot(preds, task, type = "pca")
+autoplot(preds, task, type = "pca", frame = TRUE, frame.type = "norm")
+
+# extract class assigments
+nrow(data_) == length(preds$partition)
+data_$symbol[which(preds$partition == 1)]
+data_$symbol[which(preds$partition == 2)]
+data_$symbol[which(preds$partition == 3)]
+
+
+# QC BACKTEST -------------------------------------------------------------
+# inspect
+DT_sample[year == 2023 & month == 5, unique(symbol)]
+
+# shift dates to lead month
+qc_data <- DT_sample[, .(fillingDate, symbol)]
+qc_data[, month_lead := ceiling_date(fillingDate, unit = "month")]
+qc_data[, month_last := ceiling_date(month_lead, unit = "month") - 1]
+qc_data <- qc_data[ , .(symbol = symbol, month = seq(month_lead, month_last, by = "day")), by = 1:nrow(qc_data)]
+qc_data <- qc_data[, .(month, symbol)]
+setnames(qc_data, "month", "date")
+qc_data <- unique(qc_data)
+
+# save universe on blob azure
+qc_data[, date := format.Date(date, format = "%Y%m%d")]
+qc_data_save <- qc_data[, .(symbol = paste0(unlist(symbol), collapse = ",")), by = date]
+endpoint = storage_endpoint(Sys.getenv("BLOB-ENDPOINT-SNP"), Sys.getenv("BLOB-KEY-SNP"))
+storage_write_csv(qc_data_save, storage_container(endpoint, "qc-backtest"), "universe.csv", col_names = FALSE)
+
+
+
+
 
 # SPX symbols
 utils_data <- UtilsData$new()
 sp500_symobls_by_date <- utils_data$sp500_history()
 
-############# TEST ################
-# x <- sp500_symobls_by_date[date %between% c("2001-12-22", "2002-01-01")]
-# unique(x$symbol)
-# unique(x$symbol)[!(unique(x$symbol) %in% unique(usa_symbols$symbol))]
-# unique(x$symbol)[!(unique(x$symbol) %in% unique(stocks$symbol))]
-#
-# fmp <- FMP$new()
-# fmp$get_daily("AABA", "2001-12-01", "2002-01-01")
-# y <- fmp$get_intraday_equities(symbol = "AABA", from = "2009-12-01", to = "2020-01-01")
-
-############# TEST ################
-
 # get profiles for all symbols
 # url <- "https://financialmodelingprep.com/api/v3/profile/"
 # profiles_l <- lapply(unique(usa_symbols$symbol), function(x) {
@@ -138,49 +263,27 @@ market_cap_data <- rbindlist(market_cap_data_l)
 market_cap_data[, date := as.Date(date)]
 prices_usa <- merge(prices_usa, market_cap_data, by = c("symbol", "date"), all.x = TRUE, all.y = FALSE)
 
-# fundamental data
-pin_list(board_fundamentals)
-key_metrics <- as.data.table(pin_read(board_fundamentals, "key-metrics"))
-key_metrics[, price := pfcfRatio * freeCashFlowPerShare]
-key_metrics[, `:=`(period = NULL, date = as.Date(as.character(date)), symbol = as.character(symbol))]
-fg <- as.data.table(pin_read(board_fundamentals, "financial-growth-growth"))
-fg[, `:=`(period = NULL, date = as.Date(as.character(date)), symbol = as.character(symbol))]
-is <- as.data.table(pin_read(board_fundamentals, "income-statements"))
-is[, `:=`(date = as.Date(as.character(date)), symbol = as.character(symbol), fillingDate = as.Date(as.character(fillingDate)))]
-bs <- as.data.table(pin_read(board_fundamentals, "balance-sheet-statement"))
-bs[, `:=`(period = NULL, date = as.Date(as.character(date)), symbol = as.character(symbol),
-          fillingDate = as.Date(as.character(fillingDate)))]
-cf <- as.data.table(pin_read(board_fundamentals, "cash-flow-statement"))
-cf[, `:=`(period = NULL, date = as.Date(as.character(date)), symbol = as.character(symbol),
-          fillingDate = as.Date(as.character(fillingDate)))]
-ratios <- as.data.table(pin_read(board_fundamentals, "ratios"))
-ratios[, `:=`(period = NULL, date = as.Date(as.character(date)), symbol = as.character(symbol))]
-# merge fundamntal datasets
-fundamentals <- Reduce(function(x, y) merge(x, y, by = c("symbol", "date"), all.x = TRUE, all.y = FALSE),
-                       list(is, bs, cf, key_metrics, fg, ratios))
-setorderv(fundamentals, c("symbol", "fillingDate"))
-# writexl::write_xlsx(fundamentals, "~/fundamentals.xlsx")
-
 
 
 # COARSE UNIVERSE SELECTION ------------------------------------------------------
 # number of symbols by year-month
-symbols_by_year_month <- prices_usa[, .(n_symbols = length(unique(symbol))), by = .(year, month)]
+symbols_by_year_month <- price_factors_dt[, .(n_symbols = length(unique(symbol))), by = .(year, month)]
 symbols_by_year_month[, date := as.Date(paste(year, month, "01", sep = "-"), format = "%Y-%m-%d")]
 ggplot(symbols_by_year_month, aes(date, n_symbols)) + geom_line()
 
 # change real month to lead month. we pretend we do analysis in lead month because
 # this is the month when we make universe selection decision
 
-# remove where raw close price < 5$
-min_price = 5
-prices_usa_penny_removed <- copy(prices_usa)
-prices_usa_penny_removed[, price_threshold := any(close_raw > min_price), by = c("symbol", "year", "month")]
-prices_usa_penny_removed <- prices_usa_penny_removed[price_threshold == TRUE]
-nrow(prices_usa)
-nrow(prices_usa_penny_removed)
-prices_usa_penny_removed[, n_obs_pet_month := .N, by = c("symbol", "year", "month")]
-prices_usa_penny_removed <- prices_usa_penny_removed[n_obs_pet_month > 15]
+# TODO ADD close_raw to Factors
+# # remove where raw close price < 5$
+# min_price = 5
+# prices_usa_penny_removed <- copy(price_factors_dt)
+# prices_usa_penny_removed[, price_threshold := any(close_raw > min_price), by = c("symbol", "year", "month")]
+# prices_usa_penny_removed <- prices_usa_penny_removed[price_threshold == TRUE]
+# nrow(prices_usa)
+# nrow(prices_usa_penny_removed)
+# prices_usa_penny_removed[, n_obs_pet_month := .N, by = c("symbol", "year", "month")]
+# prices_usa_penny_removed <- prices_usa_penny_removed[n_obs_pet_month > 15]
 
 # keep only stocks with positive month rturn on the begining of the month
 # help_dt = unique(prices_usa_penny_removed[, .(symbol, year, month)])
@@ -192,10 +295,10 @@ prices_usa_penny_removed <- prices_usa_penny_removed[n_obs_pet_month > 15]
 
 # keep most liquid
 n_most_liquid <- 500
-prices_usa_penny_removed[, volume_sum := sum(volume, na.rm = TRUE), by = .(symbol, year, month)]
-most_liquid <- unique(prices_usa_penny_removed[, .(symbol, year, month, volume_sum)])
+price_factors_dt[, volume_sum := sum(volume, na.rm = TRUE), by = .(symbol, year, month)]
+most_liquid <- unique(price_factors_dt[, .(symbol, year, month, volume_sum)])
 most_liquid <- most_liquid[, .SD[volume_sum %in% tail(sort(unique(volume_sum)), n_most_liquid)], by = .(year, month)]
-prices_most_liquid <- most_liquid[, `:=`(index = 1, volume_sum = NULL)][prices_usa, on = .(year = year, month = month, symbol = symbol)]
+prices_most_liquid <- most_liquid[, `:=`(index = 1, volume_sum = NULL)][price_factors_dt, on = .(year = year, month = month, symbol = symbol)]
 prices_most_liquid <- prices_most_liquid[index == 1]
 prices_most_liquid[, index := NULL]
 setorderv(prices_most_liquid, c("year", "month", "symbol"))
@@ -239,7 +342,10 @@ prices_most_liquid_least_vol[year == 2022 & month == 1, unique(symbol)]
 
 # FINE FUNDAMENTAL FILTERING  -------------------------------------------------------------
 # choose fundamnetal features
-features_fund <- fundamentals[, .(symbol, date, fillingDate, fiveYRevenueGrowthPerShare,
+features_fund = fundamental_factors[, .(symbol, date, fillingDate, fiveYRevenueGrowthPerShare,
+                        fiveYDividendperShareGrowthPerShare, payoutRatio, freeCashFlowYield)]
+
+features_fund <- fundamentals [, .(symbol, date, fillingDate, fiveYRevenueGrowthPerShare,
                                   fiveYDividendperShareGrowthPerShare, payoutRatio.x,
                                   returnOnCapitalEmployed, freeCashFlowYield)]
 features_fund[, filling_date := fillingDate]
@@ -326,592 +432,592 @@ qc_data_save <- qc_data[, .(symbol = paste0(unlist(symbol), collapse = ",")), by
 storage_write_csv(qc_data_save, storage_container(endpoint, "qc-backtest"), "universe.csv", col_names = FALSE)
 
 
-# MARKET RISK -------------------------------------------------------------
-# mcap (micro, mid, large) 52w high / 52w low
-mcap_52 <- prices_usa[, .(symbol, date, high, low, close, marketCap)]
-mcap_52[, q_33 := quantile(marketCap, probs = c(0.33), na.rm = TRUE), by = "date"]
-mcap_52[, q_66 := quantile(marketCap, probs = c(0.66), na.rm = TRUE), by = "date"]
-mcap_52 <- mcap_52[!is.na(q_33) & !is.na(q_66)]
-mcap_52[marketCap < q_33, size := "micro" ]
-mcap_52[marketCap > q_66, size := "large" ]
-mcap_52[marketCap >= q_33 & marketCap <= q_66, size := "mid" ]
-mcap_52[is.na(size)]
-setorderv(mcap_52, c("symbol", "date"))
-mcap_52[, high52w := frollapply(high, 52 * 5, max), by = "symbol"]
-mcap_52[, low52w := frollapply(low, 52 * 5, min), by = "symbol"]
-divergence <- 0.02 # PARAMETER
-mcap_52[(high52w / close  - 1) < divergence, high52w_dummy := 1]
-mcap_52[(low52w / close  - 1) > -divergence, low52w_dummy := 1]
-print(paste0("Many missing values!!! Percent of total observations: ", sum(is.na(mcap_52$marketCap)) / nrow(mcap_52) ))
-mcap_52 <- mcap_52[!is.na(marketCap)]
-mcap_52_indicator <- mcap_52[, .(high52w_sum = sum(high52w_dummy, na.rm = TRUE),
-                                 low52w_sum = sum(low52w_dummy, na.rm = TRUE)), by = c("size", "date")]
-setorderv(mcap_52_indicator, c("size", "date"))
-mcap_52_indicator[, low_high_52w_ratio := low52w_sum / high52w_sum]
-g1 <- ggplot(mcap_52_indicator, aes(x = date, y = low_high_52w_ratio, color = size)) +
-  geom_line() +
-  ggtitle("Whole period")
-g1 <- ggplot(mcap_52_indicator[date %between% DOTCOM], aes(x = date, y = low_high_52w_ratio, color = size)) +
-  geom_line() +
-  ggtitle("DOTCOM")
-g2 <- ggplot(mcap_52_indicator[date %between% GFC], aes(x = date, y = low_high_52w_ratio, color = size)) +
-  geom_line() +
-  ggtitle("GFC")
-g3 <- ggplot(mcap_52_indicator[date %between% AFTERGFCBULL], aes(x = date, y = low_high_52w_ratio, color = size)) +
-  geom_line() +
-  ggtitle("AFTERGFCBULL")
-g4 <- ggplot(mcap_52_indicator[date %between% COVID], aes(x = date, y = low_high_52w_ratio, color = size)) +
-  geom_line() +
-  ggtitle("COVID")
-(g1 / g2) | (g3 / g4)
-(g1 / g2 / g3 / g4)
-
-# volume of growth / decline stocks
-vol_52 <- prices_usa[, .(symbol, date, high, low, close, volume)]
-vol_52[, return_52w := close / shift(close, n = 52 * 5) - 1, by = "symbol"]
-vol_52[return_52w > 0, return_52w_dummy := "up", by = "symbol"]
-vol_52[return_52w <= 0, return_52w_dummy := "down", by = "symbol"]
-vol_52 <- vol_52[!is.na(return_52w_dummy)]
-vol_52_indicators <- vol_52[, .(volume = sum(volume, na.rm = TRUE)), by = c("return_52w_dummy", "date")]
-setorderv(vol_52_indicators, c("return_52w_dummy", "date"))
-vol_52_indicators <- dcast(vol_52_indicators, date ~ return_52w_dummy)
-vol_52_indicators[, volume_down_up_ratio := down / up]
-ggplot(vol_52_indicators, aes(x = date, y = volume_down_up_ratio)) +
-  geom_line() +
-  ggtitle("Whole period")
-g1 <- ggplot(vol_52_indicators[date %between% DOTCOM], aes(x = date, y = volume_down_up_ratio)) +
-  geom_line() +
-  ggtitle("DOTCOM")
-g2 <- ggplot(vol_52_indicators[date %between% GFC], aes(x = date, y = volume_down_up_ratio)) +
-  geom_line() +
-  ggtitle("GFC")
-g3 <- ggplot(vol_52_indicators[date %between% AFTERGFCBULL], aes(x = date, y = volume_down_up_ratio)) +
-  geom_line() +
-  ggtitle("AFTERGFCBULL")
-g4 <- ggplot(vol_52_indicators[date %between% COVID], aes(x = date, y = volume_down_up_ratio)) +
-  geom_line() +
-  ggtitle("COVID")
-(g1 / g2 / g3 / g4)
-
-# spread high yield / investment grade
-bonds <- prices[symbol %in% c("DHF", "DSM"), .(symbol, date, close)]
-bonds <- dcast(bonds, date ~ symbol)
-bonds$spread <- bonds[, 2] / bonds[, 3]
-ggplot(bonds, aes(x = date, y = spread)) +
-  geom_line()
-
-# VIX
-get_fread_data <- function(id = "VIXCLS") {
-  data_ <- fredr(series_id = id, observation_start = as.Date("2000-01-01"), observation_end = Sys.Date())
-  data_ <- as.data.table(data_)
-  data_ <- data_[, .(date, value)]
-  colnames(data_)[2] <- id
-  return(data_)
-}
-vix <- get_fread_data("VIXCLS")
-vix[, date := as.Date(date)]
-
-
-
-# FEATURE IMPORTANCE ------------------------------------------------------
-# merge all data
-mcap_52_indicator_ <- melt(mcap_52_indicator, id.vars = c("date", "size"))
-mcap_52_indicator_[, var := paste0(size, "_", variable)]
-mcap_52_indicator_ <- dcast(mcap_52_indicator_, date ~ var, value.var = "value")
-DT <- Reduce(function(x, y) merge(x, y, by = "date", all.x = TRUE, all.y = FALSE),
-             list(spy, vix, vol_52_indicators, mcap_52_indicator_))
-DT <- na.omit(DT)
-
-# labels for feature importance and ML
-DT[, ret_1 := shift(close, -1L, "shift") / close  - 1]
-DT[, ret_5 := shift(close, -4L, "shift") / close - 1]
-DT[, ret_22 := shift(close, -21L, "shift") / close - 1]
-DT <- na.omit(DT)
-
-# define feature cols and labels
-LABEL = "ret_1"
-features_cols <- colnames(DT)[which(colnames(DT) == "VIXCLS"):(which(colnames(DT) == "ret_1") - 1)]
-# labels <- colnames(DT)[which(colnames(DT) == "ret_1"):which(colnames(DT) == "ret_22")]
-# cols <- c(features_cols, labels)
-
-# convert integer to numeric
-cols_change <- c("up", "down")
-DT[, (cols_change) := lapply(.SD, as.numeric), .SDcols = cols_change]
-
-# add interactions
-interactions <- model.matrix( ~ .^2 - 1, data = DT[, ..features_cols])
-cols_keep <- c("date", "close", "returns", LABEL)
-DT <- cbind(DT[, ..cols_keep], interactions)
-colnames(DT) <- gsub(":", "_", colnames(DT))
-features_cols_inter <- colnames(DT)[which(colnames(DT) == "VIXCLS"):ncol(DT)]
-
-# remove inf values
-dim(DT)
-DT <- DT[is.finite(rowSums(DT[, .SD, .SDcols = is.numeric], na.rm = TRUE))] # remove inf values
-dim(DT)
-
-# remove constant columns
-remove_cols <- features_cols_inter[apply(DT[, ..features_cols_inter], 2, var, na.rm=TRUE) == 0]
-print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
-# feature_cols <- setdiff(feature_cols, remove_cols)
-
-# remove highly correlated features
-cor_matrix <- cor(DT[, ..features_cols_inter])
-cor_matrix_rm <- cor_matrix
-cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
-diag(cor_matrix_rm) <- 0
-remove_cols <- features_cols_inter[apply(cor_matrix_rm, 2, function(x) any(abs(x) > 0.98))]
-print(paste0("Removing highly correlated featue (> 0.98): ", remove_cols))
-features_cols_inter <- setdiff(features_cols_inter, remove_cols)
-
-# stationarity
-number_differences <- lapply(DT[, ..features_cols_inter],
-                             function(x) forecast::ndiffs(log(x)))
-number_differences_cols <- number_differences[number_differences > 0]
-DT[, (names(number_differences_cols)) := lapply(.SD, function(x) x - shift(x)), .SDcols = names(number_differences_cols)]
-
-# importnat features
-cols_keep <- c(features_cols_inter, LABEL)
-X <- DT[, ..cols_keep]
-X <- na.omit(X)
-X <- as.matrix(X)
-
-# f1st
-f1st_fi <- f1st(X[, ncol(X)], X[, -ncol(X)], kmn = 20, sub = TRUE)
-cov_index_f1st <- colnames(X[, -ncol(X)])[f1st_fi[[1]][, 1]]
-
-# f3st_1
-f3st_1 <- f3st(X[, ncol(X)], X[, -ncol(X)], m = 1)
-cov_index_f3st_1 <- unique(as.integer(f3st_1[[1]][1, ]))[-1]
-cov_index_f3st_1 <- cov_index_f3st_1[cov_index_f3st_1 != 0]
-cov_index_f3st_1 <- colnames(X[, -ncol(X)])[cov_index_f3st_1]
-
-# interesection of all important vars
-most_important_vars <- intersect(cov_index_f1st, cov_index_f3st_1)
-important_vars <- c(cov_index_f1st, cov_index_f3st_1)
-
-# train/test and holdout set
-start_holdout_date <- as.Date("2021-01-01")
-holdout_ids <- which(DT$date > start_holdout_date)
-X_model <- DT[-holdout_ids, ]
-X_holdout <- DT[holdout_ids, ] # TODO SAVE THIS FOR QUANTCONNECT BACKTESTING
-
-# select only labels and features
-X_model <- X_model[, .SD, .SDcols = c("date", features_cols_inter, "returns")]
-X_holdout <- X_holdout[, .SD, .SDcols = c("date", features_cols_inter, "returns")]
-
-
-
-# SIMPLE MODELS -----------------------------------------------------------
-# train data
-cols_keep <- c("date", "returns", most_important_vars)
-X_train <- X_model[, ..cols_keep]
-X_train <- na.omit(X_train)
-X_test <- na.omit(X_holdout[, ..cols_keep])
-
-# parameters
-window_lengths <- c(252, 252 * 2)
-
-# # linear regression KEEP ONLY BECAUSE IT CAN BE HELPFULL LATER
-# linear_regressions <- lapply(window_lengths, function(x) {
-#   y <- roll_regres(returns ~ ., data = X_train[, 2:ncol(X_train)], width = x, do_compute = "1_step_forecasts")
-#   y <- cbind.data.frame(date = X_train[, date], y$one_step_forecasts)
-#   colnames(y)[2] <- paste0("pred_lm_one_step_", x)
-#   y
-# })
-# preds_linear_regressions <- Reduce(function(x, y) merge(x, y, by = "date", all.x = TRUE, all.y = FALSE),
-#                                    linear_regressions)
-# preds_linear_regressions$sell <- apply(preds_linear_regressions[, 2:ncol(preds_linear_regressions)], 1, function(x) all(x < 0))
-# preds_linear_regressions <- as.data.table(preds_linear_regressions)
-# preds_linear_regressions[sell == TRUE, pred_lm_one_step_all_negative := -0.1]
-# preds_linear_regressions[sell == FALSE, pred_lm_one_step_all_negative := 0.1]
-# preds_linear_regressions$sell <- NULL
-# tail(preds_linear_regressions)
-
-
-# VAR
-library(vars)
-library(BigVAR)
-library(tsDyn)
-roll_preds <- lapply(window_lengths, function(x) {
-   runner(
-    x = X_train,
-    f = function(x) {
-      # x <- X_train[1:300, 1:ncol(X_train)]
-      y <- as.data.frame(x[, 2:ncol(x)])
-      y <- y[, which(apply(y, 2 , sd) != 0), drop = FALSE]
-      if(length(y) == 1) print("STOP!")
-
-      # VAR
-      res <- VAR(y, lag.max = 5, type = "both")
-      p <- predict(res)
-      var_pred_onestep <- p$fcst[[1]][1, 1]
-
-      # BigVar
-      Y <- as.matrix(y)
-      B = BigVAR.fit(Y, struct='Basic', p=5, lambda=1)[,,1]
-      Z = VARXLagCons(Y, p=5, oos=TRUE)$Z
-      yhat = B%*%Z[,ncol(Z),drop=FALSE]
-      bigvar_pred_onestep <- yhat[1]
-
-      # TVAR (1)
-      tv1 <- tryCatch(TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
-      if (is.null(tv1)) {
-        tv1_pred_onestep <- NA
-      } else {
-        tv1 <- predict(tv1)
-        tv1_pred_onestep <- tv1[1, 1]
-      }
-
-      # TVAR (2)
-      tv2 <- tryCatch(TVAR(y, lag=2, nthresh=2, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
-      if (is.null(tv2)) {
-        tv2_pred_onestep <- NA
-      } else {
-        tv2 <- predict(tv2)
-        tv2_pred_onestep <- tv2[1, 1]
-      }
-
-      # merge all predictions
-      data.frame(var_pred_onestep = var_pred_onestep,
-                 bigvar_pred_onestep = bigvar_pred_onestep,
-                 tv1_pred_onestep = tv1_pred_onestep,
-                 tv2_pred_onestep = tv2_pred_onestep)
-    },
-    k = x,
-    lag = 0L,
-    na_pad = TRUE
-  )
-})
-roll_preds_merged <- lapply(roll_preds, function(x) lapply(x, as.data.table))
-roll_preds_merged <- lapply(roll_preds_merged, rbindlist, fill = TRUE)
-lapply(roll_preds_merged, function(x) x[, V1 := NULL])
-roll_preds_merged <- lapply(seq_along(roll_preds_merged), function(i) {
-  colnames(roll_preds_merged[[i]]) <- paste0(colnames(roll_preds_merged[[i]]), "_", window_lengths[i])
-  roll_preds_merged[[i]]
-})
-roll_preds_merged <- as.data.table(do.call(cbind, roll_preds_merged))
-roll_preds_merged <- roll_preds_merged[, lapply(.SD, unlist)]
-roll_preds_merged$mostly_negative <- apply(roll_preds_merged, 1, function(x) sum(x < 0) > 5)
-roll_preds_merged$mostly_negative_tv2 <- apply(roll_preds_merged[, .(tv2_pred_onestep_252, tv2_pred_onestep_504)],
-                                               1, function(x) sum(x < 0) > 2)
-roll_preds_merged[, mostly_negative := ifelse(mostly_negative == TRUE, -0.1, 0.1)]
-roll_preds_merged[, mostly_negative_tv2 := ifelse(mostly_negative_tv2 == TRUE, -0.1, 0.1)]
-roll_preds_merged$mean_pred <- apply(roll_preds_merged, 1, function(x) mean(x))
-predictions_var <- cbind(date = X_train[, date], roll_preds_merged)
-
-# TVAR
-# library(tsDyn)
-# data(zeroyld)
-# tv <- TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE)
-# print(tv)
-# summary(tv)
-# plot(tv)
-# predict(tv)
-# c(AIC(tv), BIC(tv), logLik(tv))
-
-# backtest apply
-backtest <- function(returns, indicator, threshold, return_cumulative = TRUE) {
-  sides <- vector("integer", length(indicator))
-  for (i in seq_along(sides)) {
-    if (i %in% c(1) || is.na(indicator[i-1])) {
-      sides[i] <- NA
-    } else if (indicator[i-1] < threshold) { #  & indicator_2[i-1] > 1
-      sides[i] <- 0
-    } else {
-      sides[i] <- 1
-    }
-  }
-  sides <- ifelse(is.na(sides), 1, sides)
-  returns_strategy <- returns * sides
-  if (return_cumulative) {
-    return(PerformanceAnalytics::Return.cumulative(returns_strategy))
-  } else {
-    return(returns_strategy)
-  }
-}
-Performance <- function(x) {
-  cumRetx = Return.cumulative(x)
-  annRetx = Return.annualized(x, scale=252 * 8)
-  sharpex = SharpeRatio.annualized(x, scale=252 * 8)
-  winpctx = length(x[x > 0])/length(x[x != 0])
-  annSDx = sd.annualized(x, scale=252 * 8)
-
-  DDs <- findDrawdowns(x)
-  maxDDx = min(DDs$return)
-  maxLx = max(DDs$length)
-
-  Perf = c(cumRetx, annRetx, sharpex, winpctx, annSDx, maxDDx, maxLx)
-  names(Perf) = c("Cumulative Return", "Annual Return","Annualized Sharpe Ratio",
-                  "Win %", "Annualized Volatility", "Maximum Drawdown", "Max Length Drawdown")
-  return(Perf)
-}
-backtest_data <- merge(spy, predictions_var, by = "date", all.x = TRUE, all.y = FALSE)
-backtest_data <- na.omit(backtest_data)
-Return.cumulative(backtest_data$returns)
-backtest_results <- lapply(backtest_data[, 4:ncol(backtest_data)], function(x) backtest(backtest_data$returns, x, -0.002))
-backtest_results <- as.data.table(backtest_results)
-backtest_results
-
-# individual backtests
-x <- backtest(backtest_data$returns, backtest_data$tv2_pred_onestep_504, -0.002, FALSE)
-backtest_results_ind <- as.xts(cbind(SPY = backtest_data$returns, Strategy = x), order.by = as.Date(backtest_data$date))
-charts.PerformanceSummary(backtest_results_ind)
-Performance(backtest_results_ind$SPY)
-Performance(backtest_results_ind$Strategy)
-# subsample
-charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[1:1000])
-charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[1000:2000])
-charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[2000:3000])
-charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[3000:4000])
-charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[4000:nrow(backtest_data)])
-
-# prediction on test set with best model
-X_test_ <- rbind(X_train[(nrow(X_train)-504+1):nrow(X_train),], X_test)
-roll_preds_test <- lapply(window_lengths, function(x) {
-  runner(
-    x = X_test_,
-    f = function(x) {
-      # x <- X_train[1:300, 1:ncol(X_train)]
-      y <- as.data.frame(x[, 2:ncol(x)])
-      y <- y[, which(apply(y, 2 , sd) != 0), drop = FALSE]
-      if(length(y) == 1) print("STOP!")
-
-      # VAR
-      res <- VAR(y, lag.max = 5, type = "both")
-      p <- predict(res)
-      var_pred_onestep <- p$fcst[[1]][1, 1]
-
-      # BigVar
-      Y <- as.matrix(y)
-      B = BigVAR.fit(Y, struct='Basic', p=5, lambda=1)[,,1]
-      Z = VARXLagCons(Y, p=5, oos=TRUE)$Z
-      yhat = B%*%Z[,ncol(Z),drop=FALSE]
-      bigvar_pred_onestep <- yhat[1]
-
-      # TVAR (1)
-      tv1 <- tryCatch(TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
-      if (is.null(tv1)) {
-        tv1_pred_onestep <- NA
-      } else {
-        tv1 <- predict(tv1)
-        tv1_pred_onestep <- tv1[1, 1]
-      }
-
-      # TVAR (2)
-      tv2 <- tryCatch(TVAR(y, lag=2, nthresh=2, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
-      if (is.null(tv2)) {
-        tv2_pred_onestep <- NA
-      } else {
-        tv2 <- predict(tv2)
-        tv2_pred_onestep <- tv2[1, 1]
-      }
-
-      # merge all predictions
-      data.frame(var_pred_onestep = var_pred_onestep,
-                 bigvar_pred_onestep = bigvar_pred_onestep,
-                 tv1_pred_onestep = tv1_pred_onestep,
-                 tv2_pred_onestep = tv2_pred_onestep)
-    },
-    k = x,
-    lag = 0L,
-    na_pad = TRUE
-  )
-})
-predictions_test <- as.data.table(roll_preds_test)
-predictions_test <- unlist(predictions_test$V2)
-predictions_test <- cbind.data.frame(date = X_test_[, date], predictions_test)
-setnames(predictions_test, c("date", "tv2_pred_onestep_504"))
-
-backtest_data_test <- merge(spy, predictions_test, by = "date", all.x = TRUE, all.y = FALSE)
-backtest_data_test <- na.omit(backtest_data_test)
-Return.cumulative(backtest_data_test$returns)
-backtest_results_test <- lapply(backtest_data_test[, 4:ncol(backtest_data_test)], function(x) backtest(backtest_data_test$returns, x, -0.002))
-backtest_results_test <- as.data.table(backtest_results_test)
-backtest_results_test
-
-# individual backtests
-x <- backtest(backtest_data_test$returns, backtest_data_test$tv2_pred_onestep_504 , -0.0001, FALSE)
-backtest_results_test <- as.xts(cbind(SPY = backtest_data_test$returns, Strategy = x), order.by = as.Date(backtest_data_test$date))
-charts.PerformanceSummary(backtest_results_test)
-Performance(backtest_results_test$SPY)
-Performance(backtest_results_test$Strategy)
-
-# save data to blob
-predictions <- rbind(predictions_var[, .(date, tv2_pred_onestep_504)], predictions_test)
-predictions <- na.omit(predictions)
-predictions <- unique(predictions, by = "date")
-setorderv(predictions, "date")
-qc_backtest <- copy(predictions)
-# cols <- setdiff(cols_keep, "date")
-# qc_backtest[, (cols) := lapply(.SD, shift), .SDcols = cols] # VERY IMPORTANT STEP !
-qc_backtest <- na.omit(qc_backtest)
-file_name <- "D:/risks/pr/systemic_risk_v2.csv"
-fwrite(qc_backtest, file_name, col.names = FALSE, dateTimeAs = "write.csv")
-bl_endp_key <- storage_endpoint(Sys.getenv("BLOB-ENDPOINT"), key=Sys.getenv("BLOB-KEY"))
-cont <- storage_container(bl_endp_key, "qc-backtest")
-storage_upload(cont, file_name, basename(file_name))
-
-backtest_data_test <- merge(spy, qc_backtest, by = "date", all.x = TRUE, all.y = FALSE)
-backtest_data_test <- na.omit(backtest_data_test)
-Return.cumulative(backtest_data_test$returns)
-backtest_results_test <- lapply(backtest_data_test[, 4:ncol(backtest_data_test)], function(x) backtest(backtest_data_test$returns, x, -0.002))
-backtest_results_test <- as.data.table(backtest_results_test)
-backtest_results_test
-
-# individual backtests
-x <- backtest(backtest_data_test$returns, backtest_data_test$predictions_test , -0.002, FALSE)
-backtest_results_test <- as.xts(cbind(SPY = backtest_data_test$returns, Strategy = x), order.by = as.Date(backtest_data_test$date))
-charts.PerformanceSummary(backtest_results_test)
-
-
-
-# MODEL SYSTEMIC RISK -----------------------------------------------------
-# task for regression
-task_reg <- as_task_regr(X_train[, .SD, .SDcols = !c("date")], id = "reg", target = LABEL)
-task_reg_holdout <- as_task_regr(X_train[, .SD, .SDcols = !c("date")], id = "reg_holdout", target = LABEL)
-
-# select important features
-task_reg$select(most_important_vars)
-task_reg_holdout$select(most_important_vars)
-
-# descriptive analysis
-library(rpart.plot)
-learner = lrn("regr.rpart", maxdepth = 3, minbucket = 50, minsplit = 10, cp = 0.001)
-learner$param_set
-task_ <- task_reg$clone()
-learner$train(task_)
-predictins = learner$predict(task_)
-predictins$score(msr("regr.mae"))
-learner$importance()
-rpart_model <- learner$model
-rpart.plot(rpart_model)
-
-# xgboost
-learner = lrn("regr.xgboost")
-graph = po("removeconstants", ratio = 0.05) %>>%
-  # scaling
-  po("branch", options = c("nop_prep", "yeojohnson", "pca", "ica"), id = "prep_branch") %>>%
-  gunion(list(po("nop", id = "nop_prep"), po("yeojohnson"), po("pca", scale. = TRUE), po("ica"))) %>>%
-  po("unbranch", id = "prep_unbranch") %>>%
-  learner
-plot(graph)
-graph_learner = as_learner(graph)
-as.data.table(graph_learner$param_set)[1:80, .(id, class, lower, upper)]
-search_space = ps(
-  # preprocesing
-  prep_branch.selection = p_fct(levels = c("nop_prep", "yeojohnson", "pca", "ica")),
-  pca.rank. = p_int(2, 6, depends = prep_branch.selection == "pca"),
-  ica.n.comp = p_int(2, 6, depends = prep_branch.selection == "ica"),
-  yeojohnson.standardize = p_lgl(depends = prep_branch.selection == "yeojohnson"),
-  # model
-  regr.xgboost.nrounds = p_int(100, 5000),
-  regr.xgboost.eta = p_dbl(1e-4, 1),
-  regr.xgboost.max_depth = p_int(1, 8),
-  regr.xgboost.colsample_bytree = p_dbl(0.1, 1),
-  regr.xgboost.colsample_bylevel = p_dbl(0.1, 1),
-  regr.xgboost.lambda = p_dbl(0.1, 1),
-  regr.xgboost.gamma = p_dbl(1e-4, 1000),
-  regr.xgboost.alpha = p_dbl(1e-4, 1000),
-  regr.xgboost.subsample = p_dbl(0.1, 1)
-)
-# plan("multisession", workers = 4L)
-at_xgboost = auto_tuner(
-  method = "random_search",
-  learner = graph_learner,
-  resampling = rsmp("cv", folds = 3),
-  measure = msr("regr.rmse"),
-  search_space = search_space,
-  term_evals = 25
-)
-at_xgboost$train(task_reg)
-
-# inspect results
-archive <- as.data.table(at_xgboost$archive)
-archive
-preds = at_xgboost$predict(task_reg)
-preds$score(msr("regr.rmse"))
-
-# holdout extreme
-preds_holdout <- at_xgboost$predict(task_reg_holdout)
-preds_holdout$score(msrs(c("regr.rmse")))
-prediciotns_extreme_holdout <- as.data.table(preds_holdout)
-
-prediciotns_extreme_holdout_buy <- prediciotns_extreme_holdout[response > 2]
-nrow(prediciotns_extreme_holdout_buy)
-prediciotns_extreme_holdout_buy[, `:=`(truth_01 = ifelse(truth > 0, 1, 0),
-                                       response_01 = ifelse(response > 0, 1, 0))]
-mlr3measures::acc(as.factor(prediciotns_extreme_holdout_buy$truth_01),
-                  factor(prediciotns_extreme_holdout_buy$response_01, levels = c("0", "1")))
-
-
-
-
-# BACKTEST ----------------------------------------------------------------
-# backtst function
-backtest <- function(returns, indicator, threshold, return_cumulative = TRUE) {
-  sides <- vector("integer", length(indicator))
-  for (i in seq_along(sides)) {
-    if (i %in% c(1) || is.na(indicator[i-1])) {
-      sides[i] <- NA
-    } else if (indicator[i-1] > threshold) {
-      sides[i] <- 0
-    } else {
-      sides[i] <- 1
-    }
-  }
-  sides <- ifelse(is.na(sides), 1, sides)
-  returns_strategy <- returns * sides
-  if (return_cumulative) {
-    return(PerformanceAnalytics::Return.cumulative(returns_strategy))
-  } else {
-    return(returns_strategy)
-  }
-}
-
-# individual backtest
-mcap_52_indicator_merge <- dcast(mcap_52_indicator[, .(size, date, low_high_52w_ratio)], date ~ size)
-backtest_data <- merge(spy, mcap_52_indicator_merge, by = "date")
-results <- backtest(backtest_data$returns, backtest_data$micro, 5, FALSE)
-results <- cbind(spy, strategy = results)
-charts.PerformanceSummary(as.xts.data.table(results))
-
-
-
-
-
-# MLR3 PLAYGROOUND --------------------------------------------------------
-library(mlr3temporal)
-library(mlr3verse)
-library(tsbox)
+# # MARKET RISK -------------------------------------------------------------
+# # mcap (micro, mid, large) 52w high / 52w low
+# mcap_52 <- prices_usa[, .(symbol, date, high, low, close, marketCap)]
+# mcap_52[, q_33 := quantile(marketCap, probs = c(0.33), na.rm = TRUE), by = "date"]
+# mcap_52[, q_66 := quantile(marketCap, probs = c(0.66), na.rm = TRUE), by = "date"]
+# mcap_52 <- mcap_52[!is.na(q_33) & !is.na(q_66)]
+# mcap_52[marketCap < q_33, size := "micro" ]
+# mcap_52[marketCap > q_66, size := "large" ]
+# mcap_52[marketCap >= q_33 & marketCap <= q_66, size := "mid" ]
+# mcap_52[is.na(size)]
+# setorderv(mcap_52, c("symbol", "date"))
+# mcap_52[, high52w := frollapply(high, 52 * 5, max), by = "symbol"]
+# mcap_52[, low52w := frollapply(low, 52 * 5, min), by = "symbol"]
+# divergence <- 0.02 # PARAMETER
+# mcap_52[(high52w / close  - 1) < divergence, high52w_dummy := 1]
+# mcap_52[(low52w / close  - 1) > -divergence, low52w_dummy := 1]
+# print(paste0("Many missing values!!! Percent of total observations: ", sum(is.na(mcap_52$marketCap)) / nrow(mcap_52) ))
+# mcap_52 <- mcap_52[!is.na(marketCap)]
+# mcap_52_indicator <- mcap_52[, .(high52w_sum = sum(high52w_dummy, na.rm = TRUE),
+#                                  low52w_sum = sum(low52w_dummy, na.rm = TRUE)), by = c("size", "date")]
+# setorderv(mcap_52_indicator, c("size", "date"))
+# mcap_52_indicator[, low_high_52w_ratio := low52w_sum / high52w_sum]
+# g1 <- ggplot(mcap_52_indicator, aes(x = date, y = low_high_52w_ratio, color = size)) +
+#   geom_line() +
+#   ggtitle("Whole period")
+# g1 <- ggplot(mcap_52_indicator[date %between% DOTCOM], aes(x = date, y = low_high_52w_ratio, color = size)) +
+#   geom_line() +
+#   ggtitle("DOTCOM")
+# g2 <- ggplot(mcap_52_indicator[date %between% GFC], aes(x = date, y = low_high_52w_ratio, color = size)) +
+#   geom_line() +
+#   ggtitle("GFC")
+# g3 <- ggplot(mcap_52_indicator[date %between% AFTERGFCBULL], aes(x = date, y = low_high_52w_ratio, color = size)) +
+#   geom_line() +
+#   ggtitle("AFTERGFCBULL")
+# g4 <- ggplot(mcap_52_indicator[date %between% COVID], aes(x = date, y = low_high_52w_ratio, color = size)) +
+#   geom_line() +
+#   ggtitle("COVID")
+# (g1 / g2) | (g3 / g4)
+# (g1 / g2 / g3 / g4)
+#
+# # volume of growth / decline stocks
+# vol_52 <- prices_usa[, .(symbol, date, high, low, close, volume)]
+# vol_52[, return_52w := close / shift(close, n = 52 * 5) - 1, by = "symbol"]
+# vol_52[return_52w > 0, return_52w_dummy := "up", by = "symbol"]
+# vol_52[return_52w <= 0, return_52w_dummy := "down", by = "symbol"]
+# vol_52 <- vol_52[!is.na(return_52w_dummy)]
+# vol_52_indicators <- vol_52[, .(volume = sum(volume, na.rm = TRUE)), by = c("return_52w_dummy", "date")]
+# setorderv(vol_52_indicators, c("return_52w_dummy", "date"))
+# vol_52_indicators <- dcast(vol_52_indicators, date ~ return_52w_dummy)
+# vol_52_indicators[, volume_down_up_ratio := down / up]
+# ggplot(vol_52_indicators, aes(x = date, y = volume_down_up_ratio)) +
+#   geom_line() +
+#   ggtitle("Whole period")
+# g1 <- ggplot(vol_52_indicators[date %between% DOTCOM], aes(x = date, y = volume_down_up_ratio)) +
+#   geom_line() +
+#   ggtitle("DOTCOM")
+# g2 <- ggplot(vol_52_indicators[date %between% GFC], aes(x = date, y = volume_down_up_ratio)) +
+#   geom_line() +
+#   ggtitle("GFC")
+# g3 <- ggplot(vol_52_indicators[date %between% AFTERGFCBULL], aes(x = date, y = volume_down_up_ratio)) +
+#   geom_line() +
+#   ggtitle("AFTERGFCBULL")
+# g4 <- ggplot(vol_52_indicators[date %between% COVID], aes(x = date, y = volume_down_up_ratio)) +
+#   geom_line() +
+#   ggtitle("COVID")
+# (g1 / g2 / g3 / g4)
+#
+# # spread high yield / investment grade
+# bonds <- prices[symbol %in% c("DHF", "DSM"), .(symbol, date, close)]
+# bonds <- dcast(bonds, date ~ symbol)
+# bonds$spread <- bonds[, 2] / bonds[, 3]
+# ggplot(bonds, aes(x = date, y = spread)) +
+#   geom_line()
+#
+# # VIX
+# get_fread_data <- function(id = "VIXCLS") {
+#   data_ <- fredr(series_id = id, observation_start = as.Date("2000-01-01"), observation_end = Sys.Date())
+#   data_ <- as.data.table(data_)
+#   data_ <- data_[, .(date, value)]
+#   colnames(data_)[2] <- id
+#   return(data_)
+# }
+# vix <- get_fread_data("VIXCLS")
+# vix[, date := as.Date(date)]
 
 
-task = tsk("petrol")
-learner = LearnerRegrForecastVAR$new()
+#
+# # FEATURE IMPORTANCE ------------------------------------------------------
+# # merge all data
+# mcap_52_indicator_ <- melt(mcap_52_indicator, id.vars = c("date", "size"))
+# mcap_52_indicator_[, var := paste0(size, "_", variable)]
+# mcap_52_indicator_ <- dcast(mcap_52_indicator_, date ~ var, value.var = "value")
+# DT <- Reduce(function(x, y) merge(x, y, by = "date", all.x = TRUE, all.y = FALSE),
+#              list(spy, vix, vol_52_indicators, mcap_52_indicator_))
+# DT <- na.omit(DT)
+#
+# # labels for feature importance and ML
+# DT[, ret_1 := shift(close, -1L, "shift") / close  - 1]
+# DT[, ret_5 := shift(close, -4L, "shift") / close - 1]
+# DT[, ret_22 := shift(close, -21L, "shift") / close - 1]
+# DT <- na.omit(DT)
+#
+# # define feature cols and labels
+# LABEL = "ret_1"
+# features_cols <- colnames(DT)[which(colnames(DT) == "VIXCLS"):(which(colnames(DT) == "ret_1") - 1)]
+# # labels <- colnames(DT)[which(colnames(DT) == "ret_1"):which(colnames(DT) == "ret_22")]
+# # cols <- c(features_cols, labels)
+#
+# # convert integer to numeric
+# cols_change <- c("up", "down")
+# DT[, (cols_change) := lapply(.SD, as.numeric), .SDcols = cols_change]
+#
+# # add interactions
+# interactions <- model.matrix( ~ .^2 - 1, data = DT[, ..features_cols])
+# cols_keep <- c("date", "close", "returns", LABEL)
+# DT <- cbind(DT[, ..cols_keep], interactions)
+# colnames(DT) <- gsub(":", "_", colnames(DT))
+# features_cols_inter <- colnames(DT)[which(colnames(DT) == "VIXCLS"):ncol(DT)]
+#
+# # remove inf values
+# dim(DT)
+# DT <- DT[is.finite(rowSums(DT[, .SD, .SDcols = is.numeric], na.rm = TRUE))] # remove inf values
+# dim(DT)
+#
+# # remove constant columns
+# remove_cols <- features_cols_inter[apply(DT[, ..features_cols_inter], 2, var, na.rm=TRUE) == 0]
+# print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
+# # feature_cols <- setdiff(feature_cols, remove_cols)
+#
+# # remove highly correlated features
+# cor_matrix <- cor(DT[, ..features_cols_inter])
+# cor_matrix_rm <- cor_matrix
+# cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
+# diag(cor_matrix_rm) <- 0
+# remove_cols <- features_cols_inter[apply(cor_matrix_rm, 2, function(x) any(abs(x) > 0.98))]
+# print(paste0("Removing highly correlated featue (> 0.98): ", remove_cols))
+# features_cols_inter <- setdiff(features_cols_inter, remove_cols)
+#
+# # stationarity
+# number_differences <- lapply(DT[, ..features_cols_inter],
+#                              function(x) forecast::ndiffs(log(x)))
+# number_differences_cols <- number_differences[number_differences > 0]
+# DT[, (names(number_differences_cols)) := lapply(.SD, function(x) x - shift(x)), .SDcols = names(number_differences_cols)]
+#
+# # importnat features
+# cols_keep <- c(features_cols_inter, LABEL)
+# X <- DT[, ..cols_keep]
+# X <- na.omit(X)
+# X <- as.matrix(X)
+#
+# # f1st
+# f1st_fi <- f1st(X[, ncol(X)], X[, -ncol(X)], kmn = 20, sub = TRUE)
+# cov_index_f1st <- colnames(X[, -ncol(X)])[f1st_fi[[1]][, 1]]
+#
+# # f3st_1
+# f3st_1 <- f3st(X[, ncol(X)], X[, -ncol(X)], m = 1)
+# cov_index_f3st_1 <- unique(as.integer(f3st_1[[1]][1, ]))[-1]
+# cov_index_f3st_1 <- cov_index_f3st_1[cov_index_f3st_1 != 0]
+# cov_index_f3st_1 <- colnames(X[, -ncol(X)])[cov_index_f3st_1]
+#
+# # interesection of all important vars
+# most_important_vars <- intersect(cov_index_f1st, cov_index_f3st_1)
+# important_vars <- c(cov_index_f1st, cov_index_f3st_1)
+#
+# # train/test and holdout set
+# start_holdout_date <- as.Date("2021-01-01")
+# holdout_ids <- which(DT$date > start_holdout_date)
+# X_model <- DT[-holdout_ids, ]
+# X_holdout <- DT[holdout_ids, ] # TODO SAVE THIS FOR QUANTCONNECT BACKTESTING
+#
+# # select only labels and features
+# X_model <- X_model[, .SD, .SDcols = c("date", features_cols_inter, "returns")]
+# X_holdout <- X_holdout[, .SD, .SDcols = c("date", features_cols_inter, "returns")]
 
-rr = rsmp("forecastHoldout", ratio = 0.8)
-rr$instantiate(task)
-resample = resample(task, learner, rr, store_models = TRUE)
-resample$predictions()
 
-task = tsk("petrol")
-learner = LearnerRegrForecastVAR$new()
 
-rr = rsmp("RollingWindowCV", folds = 5, fixed_window = F)
-rr$instantiate(task)
-resample = resample(task, learner, rr, store_models = TRUE)
-resample$predictions()
+# # SIMPLE MODELS -----------------------------------------------------------
+# # train data
+# cols_keep <- c("date", "returns", most_important_vars)
+# X_train <- X_model[, ..cols_keep]
+# X_train <- na.omit(X_train)
+# X_test <- na.omit(X_holdout[, ..cols_keep])
+#
+# # parameters
+# window_lengths <- c(252, 252 * 2)
+#
+# # # linear regression KEEP ONLY BECAUSE IT CAN BE HELPFULL LATER
+# # linear_regressions <- lapply(window_lengths, function(x) {
+# #   y <- roll_regres(returns ~ ., data = X_train[, 2:ncol(X_train)], width = x, do_compute = "1_step_forecasts")
+# #   y <- cbind.data.frame(date = X_train[, date], y$one_step_forecasts)
+# #   colnames(y)[2] <- paste0("pred_lm_one_step_", x)
+# #   y
+# # })
+# # preds_linear_regressions <- Reduce(function(x, y) merge(x, y, by = "date", all.x = TRUE, all.y = FALSE),
+# #                                    linear_regressions)
+# # preds_linear_regressions$sell <- apply(preds_linear_regressions[, 2:ncol(preds_linear_regressions)], 1, function(x) all(x < 0))
+# # preds_linear_regressions <- as.data.table(preds_linear_regressions)
+# # preds_linear_regressions[sell == TRUE, pred_lm_one_step_all_negative := -0.1]
+# # preds_linear_regressions[sell == FALSE, pred_lm_one_step_all_negative := 0.1]
+# # preds_linear_regressions$sell <- NULL
+# # tail(preds_linear_regressions)
+#
+#
+# # VAR
+# library(vars)
+# library(BigVAR)
+# library(tsDyn)
+# roll_preds <- lapply(window_lengths, function(x) {
+#    runner(
+#     x = X_train,
+#     f = function(x) {
+#       # x <- X_train[1:300, 1:ncol(X_train)]
+#       y <- as.data.frame(x[, 2:ncol(x)])
+#       y <- y[, which(apply(y, 2 , sd) != 0), drop = FALSE]
+#       if(length(y) == 1) print("STOP!")
+#
+#       # VAR
+#       res <- VAR(y, lag.max = 5, type = "both")
+#       p <- predict(res)
+#       var_pred_onestep <- p$fcst[[1]][1, 1]
+#
+#       # BigVar
+#       Y <- as.matrix(y)
+#       B = BigVAR.fit(Y, struct='Basic', p=5, lambda=1)[,,1]
+#       Z = VARXLagCons(Y, p=5, oos=TRUE)$Z
+#       yhat = B%*%Z[,ncol(Z),drop=FALSE]
+#       bigvar_pred_onestep <- yhat[1]
+#
+#       # TVAR (1)
+#       tv1 <- tryCatch(TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
+#       if (is.null(tv1)) {
+#         tv1_pred_onestep <- NA
+#       } else {
+#         tv1 <- predict(tv1)
+#         tv1_pred_onestep <- tv1[1, 1]
+#       }
+#
+#       # TVAR (2)
+#       tv2 <- tryCatch(TVAR(y, lag=2, nthresh=2, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
+#       if (is.null(tv2)) {
+#         tv2_pred_onestep <- NA
+#       } else {
+#         tv2 <- predict(tv2)
+#         tv2_pred_onestep <- tv2[1, 1]
+#       }
+#
+#       # merge all predictions
+#       data.frame(var_pred_onestep = var_pred_onestep,
+#                  bigvar_pred_onestep = bigvar_pred_onestep,
+#                  tv1_pred_onestep = tv1_pred_onestep,
+#                  tv2_pred_onestep = tv2_pred_onestep)
+#     },
+#     k = x,
+#     lag = 0L,
+#     na_pad = TRUE
+#   )
+# })
+# roll_preds_merged <- lapply(roll_preds, function(x) lapply(x, as.data.table))
+# roll_preds_merged <- lapply(roll_preds_merged, rbindlist, fill = TRUE)
+# lapply(roll_preds_merged, function(x) x[, V1 := NULL])
+# roll_preds_merged <- lapply(seq_along(roll_preds_merged), function(i) {
+#   colnames(roll_preds_merged[[i]]) <- paste0(colnames(roll_preds_merged[[i]]), "_", window_lengths[i])
+#   roll_preds_merged[[i]]
+# })
+# roll_preds_merged <- as.data.table(do.call(cbind, roll_preds_merged))
+# roll_preds_merged <- roll_preds_merged[, lapply(.SD, unlist)]
+# roll_preds_merged$mostly_negative <- apply(roll_preds_merged, 1, function(x) sum(x < 0) > 5)
+# roll_preds_merged$mostly_negative_tv2 <- apply(roll_preds_merged[, .(tv2_pred_onestep_252, tv2_pred_onestep_504)],
+#                                                1, function(x) sum(x < 0) > 2)
+# roll_preds_merged[, mostly_negative := ifelse(mostly_negative == TRUE, -0.1, 0.1)]
+# roll_preds_merged[, mostly_negative_tv2 := ifelse(mostly_negative_tv2 == TRUE, -0.1, 0.1)]
+# roll_preds_merged$mean_pred <- apply(roll_preds_merged, 1, function(x) mean(x))
+# predictions_var <- cbind(date = X_train[, date], roll_preds_merged)
+#
+# # TVAR
+# # library(tsDyn)
+# # data(zeroyld)
+# # tv <- TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE)
+# # print(tv)
+# # summary(tv)
+# # plot(tv)
+# # predict(tv)
+# # c(AIC(tv), BIC(tv), logLik(tv))
+#
+# # backtest apply
+# backtest <- function(returns, indicator, threshold, return_cumulative = TRUE) {
+#   sides <- vector("integer", length(indicator))
+#   for (i in seq_along(sides)) {
+#     if (i %in% c(1) || is.na(indicator[i-1])) {
+#       sides[i] <- NA
+#     } else if (indicator[i-1] < threshold) { #  & indicator_2[i-1] > 1
+#       sides[i] <- 0
+#     } else {
+#       sides[i] <- 1
+#     }
+#   }
+#   sides <- ifelse(is.na(sides), 1, sides)
+#   returns_strategy <- returns * sides
+#   if (return_cumulative) {
+#     return(PerformanceAnalytics::Return.cumulative(returns_strategy))
+#   } else {
+#     return(returns_strategy)
+#   }
+# }
+# Performance <- function(x) {
+#   cumRetx = Return.cumulative(x)
+#   annRetx = Return.annualized(x, scale=252 * 8)
+#   sharpex = SharpeRatio.annualized(x, scale=252 * 8)
+#   winpctx = length(x[x > 0])/length(x[x != 0])
+#   annSDx = sd.annualized(x, scale=252 * 8)
+#
+#   DDs <- findDrawdowns(x)
+#   maxDDx = min(DDs$return)
+#   maxLx = max(DDs$length)
+#
+#   Perf = c(cumRetx, annRetx, sharpex, winpctx, annSDx, maxDDx, maxLx)
+#   names(Perf) = c("Cumulative Return", "Annual Return","Annualized Sharpe Ratio",
+#                   "Win %", "Annualized Volatility", "Maximum Drawdown", "Max Length Drawdown")
+#   return(Perf)
+# }
+# backtest_data <- merge(spy, predictions_var, by = "date", all.x = TRUE, all.y = FALSE)
+# backtest_data <- na.omit(backtest_data)
+# Return.cumulative(backtest_data$returns)
+# backtest_results <- lapply(backtest_data[, 4:ncol(backtest_data)], function(x) backtest(backtest_data$returns, x, -0.002))
+# backtest_results <- as.data.table(backtest_results)
+# backtest_results
+#
+# # individual backtests
+# x <- backtest(backtest_data$returns, backtest_data$tv2_pred_onestep_504, -0.002, FALSE)
+# backtest_results_ind <- as.xts(cbind(SPY = backtest_data$returns, Strategy = x), order.by = as.Date(backtest_data$date))
+# charts.PerformanceSummary(backtest_results_ind)
+# Performance(backtest_results_ind$SPY)
+# Performance(backtest_results_ind$Strategy)
+# # subsample
+# charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[1:1000])
+# charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[1000:2000])
+# charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[2000:3000])
+# charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[3000:4000])
+# charts.PerformanceSummary(as.xts(cbind(backtest_data$returns, x), order.by = as.Date(backtest_data$date))[4000:nrow(backtest_data)])
+#
+# # prediction on test set with best model
+# X_test_ <- rbind(X_train[(nrow(X_train)-504+1):nrow(X_train),], X_test)
+# roll_preds_test <- lapply(window_lengths, function(x) {
+#   runner(
+#     x = X_test_,
+#     f = function(x) {
+#       # x <- X_train[1:300, 1:ncol(X_train)]
+#       y <- as.data.frame(x[, 2:ncol(x)])
+#       y <- y[, which(apply(y, 2 , sd) != 0), drop = FALSE]
+#       if(length(y) == 1) print("STOP!")
+#
+#       # VAR
+#       res <- VAR(y, lag.max = 5, type = "both")
+#       p <- predict(res)
+#       var_pred_onestep <- p$fcst[[1]][1, 1]
+#
+#       # BigVar
+#       Y <- as.matrix(y)
+#       B = BigVAR.fit(Y, struct='Basic', p=5, lambda=1)[,,1]
+#       Z = VARXLagCons(Y, p=5, oos=TRUE)$Z
+#       yhat = B%*%Z[,ncol(Z),drop=FALSE]
+#       bigvar_pred_onestep <- yhat[1]
+#
+#       # TVAR (1)
+#       tv1 <- tryCatch(TVAR(y, lag=2, nthresh=1, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
+#       if (is.null(tv1)) {
+#         tv1_pred_onestep <- NA
+#       } else {
+#         tv1 <- predict(tv1)
+#         tv1_pred_onestep <- tv1[1, 1]
+#       }
+#
+#       # TVAR (2)
+#       tv2 <- tryCatch(TVAR(y, lag=2, nthresh=2, thDelay=1, trim=0.1, mTh=1, plot=FALSE), error = function(e) NULL)
+#       if (is.null(tv2)) {
+#         tv2_pred_onestep <- NA
+#       } else {
+#         tv2 <- predict(tv2)
+#         tv2_pred_onestep <- tv2[1, 1]
+#       }
+#
+#       # merge all predictions
+#       data.frame(var_pred_onestep = var_pred_onestep,
+#                  bigvar_pred_onestep = bigvar_pred_onestep,
+#                  tv1_pred_onestep = tv1_pred_onestep,
+#                  tv2_pred_onestep = tv2_pred_onestep)
+#     },
+#     k = x,
+#     lag = 0L,
+#     na_pad = TRUE
+#   )
+# })
+# predictions_test <- as.data.table(roll_preds_test)
+# predictions_test <- unlist(predictions_test$V2)
+# predictions_test <- cbind.data.frame(date = X_test_[, date], predictions_test)
+# setnames(predictions_test, c("date", "tv2_pred_onestep_504"))
+#
+# backtest_data_test <- merge(spy, predictions_test, by = "date", all.x = TRUE, all.y = FALSE)
+# backtest_data_test <- na.omit(backtest_data_test)
+# Return.cumulative(backtest_data_test$returns)
+# backtest_results_test <- lapply(backtest_data_test[, 4:ncol(backtest_data_test)], function(x) backtest(backtest_data_test$returns, x, -0.002))
+# backtest_results_test <- as.data.table(backtest_results_test)
+# backtest_results_test
+#
+# # individual backtests
+# x <- backtest(backtest_data_test$returns, backtest_data_test$tv2_pred_onestep_504 , -0.0001, FALSE)
+# backtest_results_test <- as.xts(cbind(SPY = backtest_data_test$returns, Strategy = x), order.by = as.Date(backtest_data_test$date))
+# charts.PerformanceSummary(backtest_results_test)
+# Performance(backtest_results_test$SPY)
+# Performance(backtest_results_test$Strategy)
+#
+# # save data to blob
+# predictions <- rbind(predictions_var[, .(date, tv2_pred_onestep_504)], predictions_test)
+# predictions <- na.omit(predictions)
+# predictions <- unique(predictions, by = "date")
+# setorderv(predictions, "date")
+# qc_backtest <- copy(predictions)
+# # cols <- setdiff(cols_keep, "date")
+# # qc_backtest[, (cols) := lapply(.SD, shift), .SDcols = cols] # VERY IMPORTANT STEP !
+# qc_backtest <- na.omit(qc_backtest)
+# file_name <- "D:/risks/pr/systemic_risk_v2.csv"
+# fwrite(qc_backtest, file_name, col.names = FALSE, dateTimeAs = "write.csv")
+# bl_endp_key <- storage_endpoint(Sys.getenv("BLOB-ENDPOINT"), key=Sys.getenv("BLOB-KEY"))
+# cont <- storage_container(bl_endp_key, "qc-backtest")
+# storage_upload(cont, file_name, basename(file_name))
+#
+# backtest_data_test <- merge(spy, qc_backtest, by = "date", all.x = TRUE, all.y = FALSE)
+# backtest_data_test <- na.omit(backtest_data_test)
+# Return.cumulative(backtest_data_test$returns)
+# backtest_results_test <- lapply(backtest_data_test[, 4:ncol(backtest_data_test)], function(x) backtest(backtest_data_test$returns, x, -0.002))
+# backtest_results_test <- as.data.table(backtest_results_test)
+# backtest_results_test
+#
+# # individual backtests
+# x <- backtest(backtest_data_test$returns, backtest_data_test$predictions_test , -0.002, FALSE)
+# backtest_results_test <- as.xts(cbind(SPY = backtest_data_test$returns, Strategy = x), order.by = as.Date(backtest_data_test$date))
+# charts.PerformanceSummary(backtest_results_test)
+#
 
-# multivariate
-X_train$date <- as.Date(X_train$date)
 
-task = TaskForecast$new(id = 'systemicrisk',
-                        backend = X_train,
-                        target = 'returns',
-                        date_col = 'date')
-learner = LearnerRegrForecastVAR$new()
+# # MODEL SYSTEMIC RISK -----------------------------------------------------
+# # task for regression
+# task_reg <- as_task_regr(X_train[, .SD, .SDcols = !c("date")], id = "reg", target = LABEL)
+# task_reg_holdout <- as_task_regr(X_train[, .SD, .SDcols = !c("date")], id = "reg_holdout", target = LABEL)
+#
+# # select important features
+# task_reg$select(most_important_vars)
+# task_reg_holdout$select(most_important_vars)
+#
+# # descriptive analysis
+# library(rpart.plot)
+# learner = lrn("regr.rpart", maxdepth = 3, minbucket = 50, minsplit = 10, cp = 0.001)
+# learner$param_set
+# task_ <- task_reg$clone()
+# learner$train(task_)
+# predictins = learner$predict(task_)
+# predictins$score(msr("regr.mae"))
+# learner$importance()
+# rpart_model <- learner$model
+# rpart.plot(rpart_model)
+#
+# # xgboost
+# learner = lrn("regr.xgboost")
+# graph = po("removeconstants", ratio = 0.05) %>>%
+#   # scaling
+#   po("branch", options = c("nop_prep", "yeojohnson", "pca", "ica"), id = "prep_branch") %>>%
+#   gunion(list(po("nop", id = "nop_prep"), po("yeojohnson"), po("pca", scale. = TRUE), po("ica"))) %>>%
+#   po("unbranch", id = "prep_unbranch") %>>%
+#   learner
+# plot(graph)
+# graph_learner = as_learner(graph)
+# as.data.table(graph_learner$param_set)[1:80, .(id, class, lower, upper)]
+# search_space = ps(
+#   # preprocesing
+#   prep_branch.selection = p_fct(levels = c("nop_prep", "yeojohnson", "pca", "ica")),
+#   pca.rank. = p_int(2, 6, depends = prep_branch.selection == "pca"),
+#   ica.n.comp = p_int(2, 6, depends = prep_branch.selection == "ica"),
+#   yeojohnson.standardize = p_lgl(depends = prep_branch.selection == "yeojohnson"),
+#   # model
+#   regr.xgboost.nrounds = p_int(100, 5000),
+#   regr.xgboost.eta = p_dbl(1e-4, 1),
+#   regr.xgboost.max_depth = p_int(1, 8),
+#   regr.xgboost.colsample_bytree = p_dbl(0.1, 1),
+#   regr.xgboost.colsample_bylevel = p_dbl(0.1, 1),
+#   regr.xgboost.lambda = p_dbl(0.1, 1),
+#   regr.xgboost.gamma = p_dbl(1e-4, 1000),
+#   regr.xgboost.alpha = p_dbl(1e-4, 1000),
+#   regr.xgboost.subsample = p_dbl(0.1, 1)
+# )
+# # plan("multisession", workers = 4L)
+# at_xgboost = auto_tuner(
+#   method = "random_search",
+#   learner = graph_learner,
+#   resampling = rsmp("cv", folds = 3),
+#   measure = msr("regr.rmse"),
+#   search_space = search_space,
+#   term_evals = 25
+# )
+# at_xgboost$train(task_reg)
+#
+# # inspect results
+# archive <- as.data.table(at_xgboost$archive)
+# archive
+# preds = at_xgboost$predict(task_reg)
+# preds$score(msr("regr.rmse"))
+#
+# # holdout extreme
+# preds_holdout <- at_xgboost$predict(task_reg_holdout)
+# preds_holdout$score(msrs(c("regr.rmse")))
+# prediciotns_extreme_holdout <- as.data.table(preds_holdout)
+#
+# prediciotns_extreme_holdout_buy <- prediciotns_extreme_holdout[response > 2]
+# nrow(prediciotns_extreme_holdout_buy)
+# prediciotns_extreme_holdout_buy[, `:=`(truth_01 = ifelse(truth > 0, 1, 0),
+#                                        response_01 = ifelse(response > 0, 1, 0))]
+# mlr3measures::acc(as.factor(prediciotns_extreme_holdout_buy$truth_01),
+#                   factor(prediciotns_extreme_holdout_buy$response_01, levels = c("0", "1")))
+#
+#
+#
+#
+# # BACKTEST ----------------------------------------------------------------
+# # backtst function
+# backtest <- function(returns, indicator, threshold, return_cumulative = TRUE) {
+#   sides <- vector("integer", length(indicator))
+#   for (i in seq_along(sides)) {
+#     if (i %in% c(1) || is.na(indicator[i-1])) {
+#       sides[i] <- NA
+#     } else if (indicator[i-1] > threshold) {
+#       sides[i] <- 0
+#     } else {
+#       sides[i] <- 1
+#     }
+#   }
+#   sides <- ifelse(is.na(sides), 1, sides)
+#   returns_strategy <- returns * sides
+#   if (return_cumulative) {
+#     return(PerformanceAnalytics::Return.cumulative(returns_strategy))
+#   } else {
+#     return(returns_strategy)
+#   }
+# }
+#
+# # individual backtest
+# mcap_52_indicator_merge <- dcast(mcap_52_indicator[, .(size, date, low_high_52w_ratio)], date ~ size)
+# backtest_data <- merge(spy, mcap_52_indicator_merge, by = "date")
+# results <- backtest(backtest_data$returns, backtest_data$micro, 5, FALSE)
+# results <- cbind(spy, strategy = results)
+# charts.PerformanceSummary(as.xts.data.table(results))
+#
 
-rr = rsmp("RollingWindowCV", folds = 4, fixed_window = FALSE)
-rr$instantiate(task)
-resample = resample(task, learner, rr, store_models = TRUE)
-resample$predictions()
+#
+#
+#
+# # MLR3 PLAYGROOUND --------------------------------------------------------
+# library(mlr3temporal)
+# library(mlr3verse)
+# library(tsbox)
+#
+#
+# task = tsk("petrol")
+# learner = LearnerRegrForecastVAR$new()
+#
+# rr = rsmp("forecastHoldout", ratio = 0.8)
+# rr$instantiate(task)
+# resample = resample(task, learner, rr, store_models = TRUE)
+# resample$predictions()
+#
+# task = tsk("petrol")
+# learner = LearnerRegrForecastVAR$new()
+#
+# rr = rsmp("RollingWindowCV", folds = 5, fixed_window = F)
+# rr$instantiate(task)
+# resample = resample(task, learner, rr, store_models = TRUE)
+# resample$predictions()
+#
+# # multivariate
+# X_train$date <- as.Date(X_train$date)
+#
+# task = TaskForecast$new(id = 'systemicrisk',
+#                         backend = X_train,
+#                         target = 'returns',
+#                         date_col = 'date')
+# learner = LearnerRegrForecastVAR$new()
+#
+# rr = rsmp("RollingWindowCV", folds = 4, fixed_window = FALSE)
+# rr$instantiate(task)
+# resample = resample(task, learner, rr, store_models = TRUE)
+# resample$predictions()
